- name: {{ functional_group_name }}
  description: "{{ functional_group_name }}"

  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]

      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
      disable_root: false

      write_files:
        - path: /usr/local/bin/set-ssh.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            localectl set-locale LANG={{ hostvars['localhost']['language'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi

        - path: /etc/containers/registries.conf.d/apptainer-mirror.conf
          permissions: '{{ file_mode }}'
          content: |
            # Apptainer registry mirror configuration for nvcr.io
            # This configures Pulp as a mirror with fallback to nvcr.io

            [[registry]]
            prefix = "nvcr.io"
            location = "nvcr.io"
            [[registry.mirror]]
            location = "{{ hostvars['localhost']['admin_nic_ip'] }}:2225"
            insecure = false

            [[registry]]
            prefix = "docker.io"
            location = "registry-1.docker.io"
            [[registry.mirror]]
            location = "{{ hostvars['localhost']['admin_nic_ip'] }}:2225"
            insecure = false

        - path: /usr/local/bin/configure_apptainer_registry.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/apptainer_registry_config.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "[INFO] ===== Verifying Apptainer registry mirror configuration ====="

            # Create registries.conf directory if it doesn't exist
            mkdir -p /etc/containers/registries.conf.d

            # Verify registry mirror configuration
            echo "[INFO] Registry mirror configuration:"
            if [ -f /etc/containers/registries.conf.d/apptainer-mirror.conf ]; then
                cat /etc/containers/registries.conf.d/apptainer-mirror.conf
                echo "[SUCCESS] Registry mirror configuration is in place"
            else
                echo "[ERROR] Registry mirror configuration not found"
                exit 1
            fi

            echo "[INFO] ===== Apptainer registry configuration verified ====="

        - path: /usr/local/bin/install_hpc_benchmarks.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/hpc_benchmarks_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting HPC Benchmarks Setup ====="

            # Check if Apptainer is installed
            echo "[INFO] Checking Apptainer installation..."
            if ! command -v apptainer &>/dev/null; then
                echo "[ERROR] Apptainer is not installed. Please install Apptainer first."
                exit 1
            fi
            echo "[SUCCESS] Apptainer is installed."
            apptainer --version

            # Navigate to HPC benchmarks directory
            BENCHMARKS_DIR="/hpc_benchmarks"
            mkdir -p $BENCHMARKS_DIR
            mount -t nfs {{ cloud_init_nfs_path}}/hpc_benchmarks $BENCHMARKS_DIR

            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount NFS hpc_benchmarks share. Exiting."
                exit 1
            fi

            echo "[INFO] Navigating to HPC benchmarks directory: $BENCHMARKS_DIR"

            if [ ! -d "$BENCHMARKS_DIR" ]; then
                echo "[ERROR] HPC benchmarks directory does not exist: $BENCHMARKS_DIR"
                exit 1
            fi

            cd "$BENCHMARKS_DIR" || {
                echo "[ERROR] Failed to change directory to $BENCHMARKS_DIR"
                exit 1
            }

            # Pull HPC benchmarks container
            # Registry mirror will automatically try Pulp first, then fallback to nvcr.io
            CONTAINER_IMAGE="hpc_benchmarks_25.09.sif"
            CONTAINER_REGISTRY="docker://nvcr.io/nvidia/hpc-benchmarks:25.09"

            echo "[INFO] Pulling HPC benchmarks container..."
            echo "[INFO] Registry: nvcr.io (will try Pulp mirror first, then fallback)"
            echo "[INFO] Destination: $BENCHMARKS_DIR/$CONTAINER_IMAGE"

            if [ -f "$CONTAINER_IMAGE" ]; then
                echo "[WARN] Container image already exists: $CONTAINER_IMAGE"
                echo "[INFO] Skipping download. Remove the file to re-download."
            else
                apptainer pull --disable-cache --dir "$BENCHMARKS_DIR" --name "$CONTAINER_IMAGE" "$CONTAINER_REGISTRY"

                if [ $? -eq 0 ] && [ -f "$CONTAINER_IMAGE" ]; then
                    echo "[SUCCESS] HPC benchmarks container pulled successfully."
                    ls -lh "$CONTAINER_IMAGE"
                else
                    echo "[ERROR] Failed to pull HPC benchmarks container from Pulp."
                    exit 1
                fi
            fi

            echo "===== HPC Benchmarks Setup Completed ====="

{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '{{ file_mode_600 }}'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - path: /root/ldms_sampler.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/ldms/ldms_sampler.sh.j2') | indent(12) }}
{% endif %}

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /usr/local/bin/check_slurm_controller_status.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/slurm/check_slurm_controller_status.sh.j2') | indent(12) }}

      runcmd:
        - /usr/local/bin/set-ssh.sh
        - groupadd -r {{ slurm_group_name }}
        - useradd -r -g {{ slurm_group_name }} -d {{ home_dir }} -s /sbin/nologin {{ user }}

        - mkdir -p /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm /etc/slurm/epilog.d /etc/munge /cert /var/log/track
        - echo "{{ cloud_init_nfs_path }}/cert  /cert   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm/epilog.d     /etc/slurm/epilog.d      nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ trackfile_nfs_path }}    /var/log/track       nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /etc/slurm/epilog.d/slurmd.service /usr/lib/systemd/system/
        - /usr/local/bin/check_slurm_controller_status.sh
        - /usr/local/bin/configure_apptainer_registry.sh
        - /usr/local/bin/install_hpc_benchmarks.sh
        - chown -R {{ user }}:{{ slurm_group_name }} /var/log/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/run/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool
        - chown -R {{ user }}:{{ slurm_group_name }} /var/lib/slurm
        - chown -R {{ munge_user }}:{{ munge_group }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm
        - chmod {{ file_mode_400 }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /etc/slurm/epilog.d/
        - mkdir -p /var/spool/slurmd
        - chmod {{ file_mode_755 }} /var/spool/slurmd
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool/slurmd
        - setenforce 0
        - systemctl enable firewalld
        - systemctl start firewalld
        - firewall-cmd --permanent --add-service=ssh
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/udp
        - firewall-cmd --permanent --add-port={{  slurm_conf_dict.SlurmdPort }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SlurmdPort }}/udp
        - firewall-cmd --reload
        - systemctl enable sshd
        - systemctl start sshd
        - systemctl enable munge
        - systemctl start munge
        - systemctl enable slurmd
        - systemctl start slurmd
        - systemctl daemon-reexec
        - systemctl restart sshd
        - cp /cert/pulp_webserver.crt /etc/pki/ca-trust/source/anchors && update-ca-trust
        - sed -i 's/^gpgcheck=1/gpgcheck=0/' /etc/dnf/dnf.conf

{% if hostvars['localhost']['openldap_support'] %}
        - /usr/local/bin/update_ldap_conf.sh
        - mkdir /ldapcerts
        - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /ldapcerts/* /etc/openldap/certs
        - umount /ldapcerts

        - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
        - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
        - firewall-cmd --reload

        - setenforce 0
        - authselect select sssd with-mkhomedir --force
        - sudo systemctl enable --now oddjobd.service
        - sudo systemctl enable --now sssd
        - setsebool -P authlogin_nsswitch_use_ldap on
        - setsebool -P authlogin_yubikey on
        - sudo systemctl restart sssd
        - systemctl restart sshd

{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - echo " Starting LDMS setup " | tee -a /var/log/ldms-cloudinit.log

        # Add NFS entry and mount
        - mkdir -p {{ client_mount_path }}
        - echo "{{ cloud_init_slurm_nfs_path }} {{ client_mount_path }} nfs defaults,_netdev 0 0" >> /etc/fstab
        - mount -a

        - /root/ldms_sampler.sh
{% endif %}
        - echo "Cloud-Init has completed successfully."
