- name: {{ functional_group_name }}
  description: "{{ functional_group_name }}"
  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config

      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]

      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
      disable_root: false

      write_files:
        - path: /usr/local/bin/doca-install.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/doca-install.sh.j2') | indent(12) }}

        - path: /usr/local/bin/configure-ib-network.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/configure-ib-network.sh.j2') | indent(12) }}

        - path: /usr/local/bin/set-ssh.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi
        
        - path: /root/.ssh/config
          permissions: '0600'
          content: |
            Host {{ k8s_control_ssh_patterns }}
                IdentityFile {{ k8s_client_mount_path }}/ssh/oim_rsa
                IdentitiesOnly yes

        - path: /etc/chrony.conf
          permissions: '0644'
          content: |
            server {{ cluster_boot_ip }} iburst

            driftfile /var/lib/chrony/drift
            rtcsync
            makestep 1.0 3
            logdir /var/log/chrony
            cmdport 0

        - path: /etc/modules-load.d/k8s.conf
          content: |
            br_netfilter
            overlay
            nf_conntrack
            vxlan
          permissions: '0644'

        - path: /etc/sysctl.d/k8s.conf
          content: |
            net.bridge.bridge-nf-call-iptables=1
            net.bridge.bridge-nf-call-ip6tables=1
            net.ipv4.ip_forward=1
            vm.overcommit_memory=1
            kernel.panic=10
          permissions: '0644'

        - path: /etc/fstab
          content: |
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}   {{ k8s_client_mount_path }}        nfs    noatime,nolock     0 0
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}/{% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}/etcd      /var/lib/etcd        nfs noatime,nolock 0 0
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}/{% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}/kubelet   /var/lib/kubelet     nfs noatime,nolock 0 0
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}/{% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}/kubernetes   /etc/kubernetes      nfs noatime,nolock 0 0
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}/{% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}/pod-logs   /var/log/pods      nfs noatime,nolock 0 0
            {{ k8s_nfs_server_ip }}:{{ k8s_server_share_path }}/packages   /var/lib/packages        nfs    noatime,nolock     0 0
            tmpfs   /tmp/crio-storage   tmpfs   size={{ k8s_crio_storage_size }},noatime,nodev,nosuid   0 0
          permissions: '0644'

        - path: /etc/containers/storage.conf
          content: |
            [storage]
            driver = "overlay"
            runroot = "/var/run/containers/storage"
            graphroot = "/tmp/crio-storage"
            [storage.options.overlay]
            mount_program = "/usr/bin/fuse-overlayfs"
          permissions: '0644'

        - path: /tmp/crio.conf
          permissions: '0644'
          content: |
            unqualified-search-registries = ["{{ pulp_mirror }}"]

            [[registry]]
            prefix = "docker.io"
            location = "registry-1.docker.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "ghcr.io"
            location = "ghcr.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "quay.io"
            location = "quay.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "registry.k8s.io"
            location = "registry.k8s.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "nvcr.io"
            location = "nvcr.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "public.ecr.aws"
            location = "public.ecr.aws"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

            [[registry]]
            prefix = "gcr.io"
            location = "gcr.io"
            [[registry.mirror]]
            location = "{{ pulp_mirror }}"

        - path: /tmp/kube-vip.yaml
          owner: root:root
          permissions: '0644'
          content: |
            apiVersion: v1
            kind: Pod
            metadata:
              creationTimestamp: null
              name: kube-vip
              namespace: kube-system
              uid: kube-vip-pod
            spec:
              containers:
              - args:
                - manager
                env:
                - name: vip_arp
                  value: "true"
                - name: port
                  value: "6443"
                - name: vip_nodename
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: vip_interface
                  value: vip_interface
                - name: vip_cidr
                  value: "{{ admin_netmask_bits }}"
                - name: dns_mode
                  value: first
                - name: cp_enable
                  value: "true"
                - name: cp_namespace
                  value: kube-system
                - name: svc_enable
                  value: "true"
                - name: svc_leasename
                  value: plndr-svcs-lock
                - name: vip_leaderelection
                  value: "true"
                - name: vip_leasename
                  value: plndr-cp-lock
                - name: vip_leaseduration
                  value: "5"
                - name: vip_renewdeadline
                  value: "3"
                - name: vip_retryperiod
                  value: "1"
                - name: vip_address
                  value: {{ kube_vip }}
                - name: prometheus_server
                  value: :2112
                image: ghcr.io/kube-vip/kube-vip:v0.8.9
                imagePullPolicy: IfNotPresent
                name: kube-vip
                resources: {}
                securityContext:
                  capabilities:
                    add:
                    - NET_ADMIN
                    - NET_RAW
                volumeMounts:
                - mountPath: /etc/kubernetes/admin.conf
                  name: kubeconfig
              hostAliases:
              - hostnames:
                - kubernetes
                ip: 127.0.0.1
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              volumes:
              - hostPath:
                  path: /etc/kubernetes/admin.conf
                name: kubeconfig
            status: {}

        - path: /usr/local/bin/k8s-cluster-setup.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            set -e
            kubeadm init --kubernetes-version={{ service_k8s_version }} \
              --pod-network-cidr={{ k8s_pod_network_cidr }} \
              --service-cidr={{ k8s_service_addresses }} \
              --apiserver-advertise-address={% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %} \
              --node-name {% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %} \
              --cri-socket=unix:///var/run/crio/crio.sock \
              --control-plane-endpoint={% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}:6443 \
              --apiserver-cert-extra-sans {{ kube_vip }}
        
        - path: /tmp/generate-control-plane-join.sh
          permissions: '0744'
          content: |
            #!/bin/bash
            set -e
            # Shared mount path where the control-plane join command will be saved
            K8S_CLIENT_MOUNT_PATH="{{ k8s_client_mount_path }}"
            mkdir -p "$K8S_CLIENT_MOUNT_PATH"
            echo "Generating Kubernetes control-plane join command..."
            # Generate certificate key and control plane join command
            CERT_KEY=$(kubeadm init phase upload-certs --upload-certs | tail -1 | tr -d '\r\n ')
            if [ -n "$CERT_KEY" ]; then
                CONTROL_PLANE_JOIN_CMD=$(kubeadm token create --ttl 0 --print-join-command --certificate-key "$CERT_KEY")
                echo "$CONTROL_PLANE_JOIN_CMD" > "${K8S_CLIENT_MOUNT_PATH}/control-plane-join-command.sh"
                chmod 644 "${K8S_CLIENT_MOUNT_PATH}/control-plane-join-command.sh"
                echo "Saved control-plane join command to: ${K8S_CLIENT_MOUNT_PATH}/control-plane-join-command.sh"
            else
                echo "ERROR: Certificate key is empty! Cannot generate control-plane join command."
                exit 1
            fi
            echo "Control-plane join script is ready. You can rerun this script anytime to refresh the command."

        - path: /usr/local/bin/install-helm.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            set -e
            HELM_VERSION="v3.19.0"
            ARCH="amd64"
            cp {{ k8s_client_mount_path }}/helm/linux-${ARCH}/helm /usr/local/bin/helm
            chmod +x /usr/local/bin/helm

            # Optional: Set up bash completion
            /usr/local/bin/helm completion bash > /etc/bash_completion.d/helm.sh
            chmod 0755 /etc/bash_completion.d/helm.sh

        - path: /tmp/ipaddress_pool.yaml
          owner: root:root
          permissions: '0644'
          content: |
            apiVersion: metallb.io/v1beta1
            kind: IPAddressPool
            metadata:
              name: first-pool
              namespace: metallb-system
            spec:
              addresses:
              - {{ pod_external_ip_range }}

        - path: /tmp/l2advertisement.yaml
          owner: root:root
          permissions: '0644'
          content: |
            apiVersion: metallb.io/v1beta1
            kind: L2Advertisement
            metadata:
              name: default
              namespace: metallb-system
            spec:
              ipAddressPools:
              - first-pool

{% if hostvars['localhost']['idrac_telemetry_support'] or hostvars['localhost']['ldms_support'] %}
        - path: /root/telemetry.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/telemetry/telemetry.sh.j2') | indent(12) }}
{% endif %}

      runcmd:
        - /usr/local/bin/set-ssh.sh
        - "systemctl enable chronyd"
        - "systemctl restart chronyd"
        - "chronyc sources"
        - "chronyc -a makestep"
        - sudo swapoff -a
        - sudo sed -i '/ swap / s/^/#/' /etc/fstab
        - sudo setenforce 0 || true
        - sudo sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config
        
        # Enable and start firewalld
        - systemctl enable firewalld
        - systemctl start firewalld

        # Open essential ports
        - firewall-cmd --permanent --add-port=22/tcp
        - firewall-cmd --permanent --add-port=6443/tcp
        - firewall-cmd --permanent --add-port=2379-2380/tcp
        - firewall-cmd --permanent --add-port=10250/tcp
        - firewall-cmd --permanent --add-port=10251/tcp
        - firewall-cmd --permanent --add-port=10252/tcp
        - firewall-cmd --permanent --add-port=10257/tcp
        - firewall-cmd --permanent --add-port=10259/tcp

        # CNI-related ports if running workloads on control plane (NodePort, CNI, etc.)
        - firewall-cmd --permanent --add-port=30000-32767/tcp
        - firewall-cmd --permanent --add-port=179/tcp
        - firewall-cmd --permanent --add-port=4789/udp
        - firewall-cmd --permanent --add-port=5473/tcp
        - firewall-cmd --permanent --add-port=51820/udp
        - firewall-cmd --permanent --add-port=51821/udp
        - firewall-cmd --permanent --add-port=9100/tcp
        - firewall-cmd --permanent --add-port=7472/tcp
        - firewall-cmd --permanent --add-port=7472/udp
        - firewall-cmd --permanent --add-port=7946/tcp
        - firewall-cmd --permanent --add-port=7946/udp
        - firewall-cmd --permanent --add-port=9090/tcp
        - firewall-cmd --permanent --add-port=8080/tcp
        
        # Enable services
        - firewall-cmd --permanent --add-service=http
        - firewall-cmd --permanent --add-service=https

        # Add pod/service networks
        - firewall-cmd --permanent --zone=trusted --add-source={{ k8s_service_addresses }}
        - firewall-cmd --permanent --zone=trusted --add-source={{ k8s_pod_network_cidr }}
        
        # Set default zone to trusted
        - firewall-cmd --set-default-zone=trusted
        
        # Reload the firewall rules
        - firewall-cmd --reload

        - sudo modprobe br_netfilter || true
        - sudo modprobe overlay || true
        - sudo modprobe nf_conntrack || true
        - sudo modprobe vxlan || true
        - sysctl --system
        - mkdir -p /tmp/crio-storage {{ k8s_client_mount_path }} /var/lib/etcd  /var/lib/kubelet /etc/kubernetes /var/log/pods /var/lib/packages
        - |
          tmpfile=$(mktemp)
          # Extract the first 'search' line only (ignore duplicates)
          search_line=$(grep '^search' /etc/resolv.conf | head -n1)
          [ -n "$search_line" ] && echo "$search_line" > "$tmpfile"

          # Add your new nameserver entries
          {% for ns in dns %}
          echo "nameserver {{ ns }}" >> "$tmpfile"
          {% endfor %}

          # Add remaining lines except search and empty lines
          grep -v '^search' /etc/resolv.conf | grep -v '^$' >> "$tmpfile"

          # Remove duplicate lines
          awk '!seen[$0]++' "$tmpfile" > /etc/resolv.conf
        - |
          if command -v chattr >/dev/null 2>&1; then
            chattr +i /etc/resolv.conf || true
          fi
        - mount -a
        - cp {{ k8s_client_mount_path }}/pulp_webserver.crt /etc/pki/ca-trust/source/anchors
        - update-ca-trust extract
        - sed -i 's/^gpgcheck=1/gpgcheck=0/' /etc/dnf/dnf.conf
        - bash /usr/local/bin/doca-install.sh && bash /usr/local/bin/configure-ib-network.sh
        - systemctl start crio.service
        - systemctl enable crio.service
        - sudo systemctl enable --now kubelet
        - mv /tmp/crio.conf /etc/containers/registries.conf.d/crio.conf
        - mv /tmp/generate-control-plane-join.sh {{ k8s_client_mount_path }}
        - systemctl daemon-reload
        - systemctl restart crio
        - kubeadm config images pull --kubernetes-version={{ service_k8s_version }}
        - echo "Installing helm"
        - /usr/local/bin/install-helm.sh

        - |
          echo "Installing Necessary Python pip packages"
          python3 -m ensurepip

          PACKAGES=({% for pkg in k8s_pip_packages %}"{{ pkg }}"{% if not loop.last %} {% endif %}{% endfor %})

          for pkg in "${PACKAGES[@]}"; do
              echo "Installing $pkg from offline repo..."
              pip3 install "$pkg" \
                  --find-links="{{ offline_pip_module_path }}/${pkg}/" \
                  --trusted-host "{{ pulp_server_ip }}" \
                  --no-index
          done
          MARKER="/etc/kubernetes/.cluster_initialized"
          export KUBECONFIG="/etc/kubernetes/admin.conf"
          if [ ! -f "$MARKER" ]; then
            # FIRST BOOT - CLUSTER INIT
            # -- All the commands below this line should be run ONCE ONLY:
            echo "Initial boot - initializing and setting up service_kube_control_plane_first_x86_64"
            mv /tmp/ipaddress_pool.yaml {{ k8s_client_mount_path }}/metallb/ipaddress_pool.yaml
            mv /tmp/l2advertisement.yaml {{ k8s_client_mount_path }}/metallb/l2advertisement.yaml
            # Setup Kubernetes cluster
            rm -rf /var/lib/etcd/*  /var/lib/kubelet/* /etc/kubernetes/*
            rm -rf /var/lib/etcd/.*  /var/lib/kubelet/.* /etc/kubernetes/.*
            #!/bin/bash
            NODE_IP="{% raw %}{{ ds.meta_data.instance_data.local_ipv4 }}{% endraw %}"
            # Find the interface with this IP
            VIP_IFACE=$(ip -o addr show | awk -v ip="$NODE_IP" '$4 ~ ip {print $2}')
            # Replace the vip_interface placeholder in the yaml
            sed -i "s/value: vip_interface/value: ${VIP_IFACE}/" /tmp/kube-vip.yaml
            mkdir -p /etc/kubernetes/manifests/
            cp /tmp/kube-vip.yaml /etc/kubernetes/manifests/kube-vip.yaml

            /usr/local/bin/k8s-cluster-setup.sh || true
            mkdir -p $HOME/.kube
            cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
            chown $(id -u):$(id -g) $HOME/.kube/config
  
            echo "Updating strictARP to true in kube-proxy configmap"
            kubectl get configmap kube-proxy -n kube-system -o yaml | \
            sed -e "s/strictARP: false/strictARP: true/" | \
            kubectl apply -f - -n kube-system
            cp {{ k8s_client_mount_path }}/calico/{{ calico_package }}.yml {{ k8s_client_mount_path }}/calico/updated-{{ calico_package }}.yml

            CALICO_YAML="{{ k8s_client_mount_path }}/calico/updated-{{ calico_package }}.yml"
            ADMIN_NIC_CIDR="{{ admin_nic_cidr }}"

            # Only add if not already present
            if ! grep -q 'name: IP_AUTODETECTION_METHOD' "$CALICO_YAML"; then
              sed -i '/value: "autodetect"/a\            - name: IP_AUTODETECTION_METHOD\n              value: "cidr='"$ADMIN_NIC_CIDR"'"' "$CALICO_YAML"
              echo "IP_AUTODETECTION_METHOD set to $ADMIN_NIC_CIDR in $CALICO_YAML"
            else
              echo "IP_AUTODETECTION_METHOD already present in $CALICO_YAML"
            fi
            
            # To apply the Calico manifest
            kubectl apply -f "$CALICO_YAML"

            export KUBECONFIG=/etc/kubernetes/admin.conf
            echo "Waiting for one Ready control plane node ..."
            # Loop until we have at least one control plane node with status Ready
            while true; do
              control_plane_ready=$(kubectl get nodes -l node-role.kubernetes.io/control-plane="" --no-headers 2>/dev/null | awk '$2=="Ready"' | wc -l)
              if [ "$control_plane_ready" -ge 1 ]; then
                echo "Found $control_plane_ready Ready control plane node(s)!"
                break
              else
                echo "No Ready control plane node yet, waiting 5s ..."
                sleep 5
              fi
            done
            # Wait for all pods in all namespaces to be ready (status=Running or Completed)
            echo "Waiting for all pods to be Ready (Running/Completed)..."
            while true; do
              not_ready=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | awk '{ print $4 }' | grep -vE '^(Running|Completed)$' | wc -l)
              if [ "${not_ready}" -eq 0 ]; then
                echo "All pods are Running or Completed."
                break
              else
                echo "$not_ready pods not yet ready, waiting 5s ..."
                sleep 5
              fi
            done

            set -e
            echo "Updating the arguments for kube-controller-manager"
            MANIFEST="/etc/kubernetes/manifests/kube-controller-manager.yaml"
            BACKUP="/tmp/kube-controller-manager.yaml"

            ARGS=(
              "--node-monitor-period=5s"
              "--node-monitor-grace-period=40s"
              "--node-eviction-rate=1"
              "--secondary-node-eviction-rate=1"
              "--terminated-pod-gc-threshold=50"
            )

            echo "Backing up kube-controller-manager manifest..."
            cp -a "$MANIFEST" "$BACKUP"
            
            # -----------------------------------------
            # Update --controllers= argument
            # -----------------------------------------
            OLD="--controllers=*,bootstrapsigner,tokencleaner"
            NEW="--controllers=*,nodeipam,nodelifecycle,bootstrapsigner,tokencleaner"

            echo "Checking and updating controllers argument in: $BACKUP"

            # Detect ANY existing --controllers= line (with or without OLD)
            if grep -Fq -- "--controllers=" "$BACKUP"; then
              echo "Existing controllers line found. Updating..."
              # Replace entire existing controllers argument safely
              sed -i "s|.*--controllers=.*|    - $NEW|" "$BACKUP"
            else
              echo "No controllers line found. Adding new one..."
              # Insert after the kube-controller-manager entry
              sed -i "/- kube-controller-manager/a \ \ \ \ - $NEW" "$BACKUP"
            fi


            for ARG in "${ARGS[@]}"; do
              if grep -Fq -- "$ARG" "$BACKUP"; then
                echo "Already present: $ARG"
              else
                echo "Adding: $ARG"
                sed -i "/- kube-controller-manager/a \ \ \ \ - $ARG" "$BACKUP"
              fi
            done
            yes | cp -i  "$BACKUP" "$MANIFEST"


            echo "All arguments processed successfully."
            echo "kubelet will auto-restart kube-controller-manager within 30-60 seconds."

            echo "Waiting for Kubernetes API..."
            until kubectl get nodes >/dev/null 2>&1; do
              sleep 10
            done
            echo "Updating the kubelet arguments."
            sed -i 's/^shutdownGracePeriod:.*/shutdownGracePeriod: 30s/' /var/lib/kubelet/config.yaml
            sed -i 's/^shutdownGracePeriodCriticalPods:.*/shutdownGracePeriodCriticalPods: 10s/' /var/lib/kubelet/config.yaml
            systemctl daemon-reload
            systemctl restart kubelet

            echo "Updating coredns config map"
            cfg="/tmp/coredns-config.yml"

            # Export CoreDNS ConfigMap into the file
            kubectl -n kube-system get configmap coredns -o yaml > "$cfg"

            # Patch: append nameservers after /etc/resolv.conf using Jinja list "dns"
            sed -i 's|/etc/resolv.conf|/etc/resolv.conf{% for ns in dns %} {{ ns }}{% endfor %}|' "$cfg"

            # Apply the patched ConfigMap
            kubectl apply -f "$cfg"

            # Restart CoreDNS deployment
            kubectl -n kube-system rollout restart deployment coredns

            # Wait for all pods in all namespaces to be ready (status=Running or Completed)
            echo "Waiting for all pods to be Ready (Running/Completed)..."
            while true; do
              not_ready=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | awk '{ print $4 }' | grep -vE '^(Running|Completed)$' | wc -l)
              if [ "${not_ready}" -eq 0 ]; then
                echo "All pods are Running or Completed."
                break
              else
                echo "$not_ready pods not yet ready, waiting 5s ..."
                sleep 5
              fi
            done

            echo "Listing all Kubernetes nodes:"
            kubectl get nodes -o wide
            echo "Listing all Kubernetes pods in all namespaces:"
            kubectl get pods --all-namespaces -o wide

            kube_vip="{{ kube_vip }}"
            kubectl get configmap kubeadm-config -n kube-system -o yaml > /tmp/kubeadm-config.yaml
            if grep -q 'controlPlaneEndpoint:' /tmp/kubeadm-config.yaml; then
              sed -i "s|controlPlaneEndpoint:.*|controlPlaneEndpoint: ${kube_vip}:6443|" /tmp/kubeadm-config.yaml
            else
              # Use correct YAML key capitalization!
              sed -i "/ClusterConfiguration:/a\    controlPlaneEndpoint: ${kube_vip}:6443" /tmp/kubeadm-config.yaml
            fi
            kubectl apply -f /tmp/kubeadm-config.yaml
 
            # Update cluster-info
            kubectl get configmap cluster-info -n kube-public -o yaml > /tmp/cluster-info.yaml
            sed -i "s|server: https://.*:6443|server: https://${kube_vip}:6443|" /tmp/cluster-info.yaml
            kubectl apply -f /tmp/cluster-info.yaml
            
            VIP="{{ kube_vip }}"
            KUBE_PORT="6443"
            sed -i "s|server: https://[^:]*:${KUBE_PORT}|server: https://${VIP}:${KUBE_PORT}|" /etc/kubernetes/admin.conf
            KUBECONFIG="${HOME}/.kube/config"
            if [ -f "$KUBECONFIG" ]; then
              sed -i "s|server: https://[^:]*:${KUBE_PORT}|server: https://${VIP}:${KUBE_PORT}|" "$KUBECONFIG"
            fi
            cp /etc/kubernetes/admin.conf $HOME/.kube/config
            mkdir -p /root/.kube
            cp /etc/kubernetes/admin.conf /root/.kube/config
            
            # Update kube-proxy configmap
            kubectl -n kube-system get configmap kube-proxy -o yaml > /tmp/kube-proxy.yaml

            # Update API server endpoint inside kubeconfig
            sed -i "s|server: https://.*:6443|server: https://${kube_vip}:6443|" /tmp/kube-proxy.yaml

            # Apply updated configmap
            kubectl apply -f /tmp/kube-proxy.yaml

            # Restart kube-proxy pods to load new config
            kubectl delete pod -n kube-system -l k8s-app=kube-proxy

            systemctl restart kubelet

            KUBE_VIP="{{ kube_vip }}"
            if kubectl config view --minify | grep -q "server: https://${KUBE_VIP}:6443"; then
              echo "SUCCESS: kube_vip (${KUBE_VIP}) is set in kubeconfig."
              echo "Running: kubeadm init phase certs apiserver --control-plane-endpoint ${KUBE_VIP}:6443"
              kubeadm init phase certs apiserver --control-plane-endpoint ${KUBE_VIP}:6443

            else
              echo "FAIL: kube_vip (${KUBE_VIP}) is NOT set in kubeconfig."
            fi

            K8S_CLIENT_MOUNT_PATH="{{ k8s_client_mount_path }}"

            # Get the certificate key
            CERT_KEY=$(kubeadm init phase upload-certs --upload-certs | tail -1 | tr -d '\r\n ')
            if [ -n "$CERT_KEY" ]; then
              CONTROL_PLANE_JOIN_CMD=$(kubeadm token create --ttl 0 --print-join-command --certificate-key "$CERT_KEY")
              echo "$CONTROL_PLANE_JOIN_CMD" > "${K8S_CLIENT_MOUNT_PATH}/control-plane-join-command.sh"
              echo "Saved control-plane join command to: ${K8S_CLIENT_MOUNT_PATH}/control-plane-join-command.sh"
            else
              echo "ERROR: Certificate key is empty! Cannot generate control-plane join command."
              exit 1
            fi

            # For joining worker nodes (regular join command)
            WORKER_JOIN_CMD=$(kubeadm token create --ttl 0 --print-join-command)
            echo "$WORKER_JOIN_CMD" > "${K8S_CLIENT_MOUNT_PATH}/worker-join-command.sh"
            echo "Saved worker join command to:       ${K8S_CLIENT_MOUNT_PATH}/worker-join-command.sh"

            
            export KUBECONFIG=/etc/kubernetes/admin.conf

            echo "Waiting for at least one READY Kubernetes worker node ..."
            while true; do
              # List nodes, exclude master/control-plane, look for Ready
              if kubectl get nodes --no-headers | grep -Ev 'control-plane|master' | grep ' Ready '; then
                echo "Worker node(s) present and Ready."
                break
              else
                echo "No Ready worker node detected yet. Retrying in 10 seconds..."
                sleep 10
              fi
            done

            #update the kubelet config.yaml
            CONFIG_FILE="/var/lib/kubelet/config.yaml"

            # Update or add the parameters
            sed -i 's|^nodeStatusUpdateFrequency:.*|nodeStatusUpdateFrequency: 10s|' $CONFIG_FILE
            sed -i 's|^nodeStatusReportFrequency:.*|nodeStatusReportFrequency: 60s|' $CONFIG_FILE
            sed -i 's|^syncFrequency:.*|syncFrequency: 60s|' $CONFIG_FILE

            # If a key is missing, append it
            grep -q "^nodeStatusUpdateFrequency:" $CONFIG_FILE || echo "nodeStatusUpdateFrequency: 10s" >> $CONFIG_FILE
            grep -q "^nodeStatusReportFrequency:" $CONFIG_FILE || echo "nodeStatusReportFrequency: 60s" >> $CONFIG_FILE
            grep -q "^syncFrequency:" $CONFIG_FILE || echo "syncFrequency: 60s" >> $CONFIG_FILE

            # Restart kubelet to apply changes
            systemctl restart kubelet

            echo "Installing plugins"
            echo "Installing nfs-client-provisioner"
            /usr/local/bin/helm install nfs-client {{ k8s_client_mount_path }}/nfs-client-provisioner/{{ nfs_subdir_external_provisioner_pkg }}.tar.gz \
              --namespace default --create-namespace \
              --set nfs.server={{ k8s_nfs_server_ip }} \
              --set nfs.path={{ k8s_server_share_path }} \
              --set storageClass.defaultClass=true \
              --set storageClass.reclaimPolicy=Retain
            echo "Waiting for nfs-subdir-external-provisioner pods to appear..."
            # Give controller some time to create pods
            sleep 15

            # Wait only if pods exist
            if kubectl get pods -n default -l app=nfs-subdir-external-provisioner | grep -q nfs-subdir; then
              kubectl wait --for=condition=Ready pod -l app=nfs-subdir-external-provisioner -n default --timeout=300s || true
            else
              echo "Pods not yet created, retrying after 10 seconds..."
              sleep 10
              kubectl wait --for=condition=Ready pod -l app=nfs-subdir-external-provisioner -n default --timeout=300s || true
            fi

            echo "Installing Metallb"
            kubectl create -f {{ k8s_client_mount_path }}/metallb/{{ metallb_package }}.yml
            echo "Waiting for MetalLB pods to be ready..."
            kubectl wait --namespace metallb-system --for=condition=Ready pods --all --timeout=300s
            echo "Waiting for MetalLB webhook to be ready..."
            until kubectl get endpoints metallb-webhook-service -n metallb-system \
              -o jsonpath='{.subsets[*].addresses[*].ip}' | grep -qE '[0-9]'; do
              echo "Webhook endpoints not ready yet. Retrying in 5s..."
              sleep 5
            done
            echo "MetalLB webhook is ready."
            echo "Deploy ipaddress pool"
            kubectl create -f {{ k8s_client_mount_path }}/metallb/ipaddress_pool.yaml
            echo "Deploy Layer2 Configuration"
            kubectl create -f {{ k8s_client_mount_path }}/metallb/l2advertisement.yaml

            #echo "Deploy Multus"
            #kubectl apply -f {{ k8s_client_mount_path }}/multus/{{ multus_package }}.yml
            #echo "Waiting for multus pods to be ready..."
            #kubectl wait --for=condition=Ready pod -l app=multus -n kube-system --timeout=300s

            #echo "Deploy Wherabouts"
            #kubectl apply -f {{ k8s_client_mount_path }}/whereabouts/whereabouts/doc/crds/daemonset-install.yaml
            #kubectl apply -f {{ k8s_client_mount_path }}/whereabouts/whereabouts/doc/crds/whereabouts.cni.cncf.io_ippools.yaml
            #kubectl apply -f {{ k8s_client_mount_path }}/whereabouts/whereabouts/doc/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml
            #echo "Waiting for whereabouts pods to be ready..."
            #kubectl wait --for=condition=Ready pod -l app=whereabouts -n kube-system --timeout=300s


            export KUBECONFIG=/etc/kubernetes/admin.conf
            echo "Display nodes and pods status after deploying plugins"
            echo "Listing all Kubernetes nodes:"
            kubectl get nodes -o wide

            echo "Listing all Kubernetes pods in all namespaces:"
            kubectl get pods --all-namespaces -o wide

            echo "Rollout and Restart coredns"
            kubectl rollout restart deployment coredns -n kube-system
            echo "Waiting for coredns pods to appear.."
            sleep 30
            kubectl wait --for=condition=Ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s

            CSI_DRIVER_SUPPORT="{{ csi_driver_powerscale_support | lower }}"
            echo "===== Checking if PowerScale CSI driver support is enabled ====="

            if [ "$CSI_DRIVER_SUPPORT" != "true" ]; then
              echo "PowerScale CSI driver support is disabled. Skipping deployment."
              true
            else
              echo "PowerScale CSI driver support is enabled. Proceeding with deployment."
              echo "===== Copying CSI PowerScale driver from NFS-mounted path ====="
              mkdir -p /opt/omnia
              POWERSCALE_DEPLOYMENT_FAILED=0

              if cp -rp {{ k8s_client_mount_path }}/csi-driver-powerscale /opt/omnia/; then
                echo "Copied CSI PowerScale driver to /opt/omnia successfully."
              else
                echo "ERROR: Failed to copy PowerScale driver. Skipping PowerScale deployment."
                POWERSCALE_DEPLOYMENT_FAILED=1
              fi

              if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                SECRET_FILE="/opt/omnia/csi-driver-powerscale/secret.yaml"
                echo "Checking if creds are provided by user"

                if [[ -f "$SECRET_FILE" ]]; then
                  echo "Found secret file at $SECRET_FILE"
                  csi_username=$(grep -v '^[[:space:]]*#' "$SECRET_FILE" | grep 'username:' | head -1 | awk -F':' '{gsub(/^[[:space:]]+|[[:space:]]+$/, "", $2); print $2}' | base64 --decode 2>/dev/null)
                  csi_password=$(grep -v '^[[:space:]]*#' "$SECRET_FILE" | grep 'password:' | head -1 | awk -F':' '{gsub(/^[[:space:]]+|[[:space:]]+$/, "", $2); print $2}' | base64 --decode 2>/dev/null)

                  if [ -z "${csi_username}" ] || [ -z "${csi_password}" ]; then
                    echo " ERROR: CSI credentials not defined in secret.yaml."
                    POWERSCALE_DEPLOYMENT_FAILED=1
                  else
                    export csi_username
                    export csi_password
                  fi
                else
                  echo "ERROR: secret.yaml not found at $SECRET_FILE."
                  POWERSCALE_DEPLOYMENT_FAILED=1
                fi
              fi

              if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                echo "===== Checking if PowerScale driver is deployed ====="
                if kubectl get pods -n isilon --no-headers 2>/dev/null | grep -q '^isilon-'; then
                  echo "PowerScale driver is already deployed on the cluster."
                  POWERSCALE_DEPLOYMENT_FAILED=1
                fi
              fi

              if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                echo "===== Checking Helm installation ====="
                if ! command -v helm >/dev/null 2>&1; then
                  echo "Helm not found. Installing..."
                  /usr/local/bin/install-helm.sh || POWERSCALE_DEPLOYMENT_FAILED=1
                fi
              fi

              if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                echo "===== Extracting PowerScale host from secret.yaml ====="
                powerscale_endpoint=$(grep '^[[:space:]]*endpoint:' "$SECRET_FILE" | head -1 | awk -F'"' '{print $2}')
                powerscale_host=$(echo "$powerscale_endpoint" | sed -E 's#https?://##' | sed -E 's#/.*##')

                echo "Extracted PowerScale Host: $powerscale_host"
                echo "===== Checking connectivity to PowerScale host ====="
                if ping -c 1 "$powerscale_host" >/dev/null 2>&1; then
                  echo "PowerScale Host ($powerscale_host) is reachable."
                else
                  echo "ERROR: PowerScale Host ($powerscale_host) is NOT reachable."
                  POWERSCALE_DEPLOYMENT_FAILED=1
                fi
              fi

              if [ "$POWERSCALE_DEPLOYMENT_FAILED" -ne 0 ]; then
                echo "PowerScale prerequisites failed. Skipping remaining deployment steps."
                true
              else
                echo "===== Ensuring 'isilon' namespace exists ====="
                kubectl create namespace isilon --dry-run=client -o yaml | kubectl apply -f - || {
                  echo "ERROR: Failed to create or verify 'isilon' namespace."
                  POWERSCALE_DEPLOYMENT_FAILED=1
                }

                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Creating and patching isilon-creds secret ====="
                  kubectl delete secret isilon-creds -n isilon >/dev/null 2>&1 || true
                  kubectl create secret generic isilon-creds -n isilon \
                    --from-file=config="$SECRET_FILE" >/dev/null 2>&1 || POWERSCALE_DEPLOYMENT_FAILED=1

                  if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                    kubectl get secret isilon-creds -n isilon -o jsonpath='{.data.config}' | base64 -d > /tmp/decoded_config.yaml 2>/dev/null
                    awk -v user="$csi_username" -v pass="$csi_password" '
                      /^[[:space:]]*#/ {print; next}
                      /^ *username:/ {sub(/:.*/, ": " user)}
                      /^ *password:/ {sub(/:.*/, ": " pass)}
                      {print}
                    ' /tmp/decoded_config.yaml > /tmp/updated_config.yaml
                    encoded_config=$(base64 -w 0 /tmp/updated_config.yaml)
                    kubectl patch secret isilon-creds -n isilon \
                      --type merge \
                      -p "{\"data\":{\"config\":\"${encoded_config}\"}}" >/dev/null 2>&1 || POWERSCALE_DEPLOYMENT_FAILED=1
                    rm -f /tmp/decoded_config.yaml /tmp/updated_config.yaml
                    echo "isilon-creds secret created and patched successfully."
                  fi
                fi

                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Applying empty certificate secret ====="
                  if [ -f "/opt/omnia/csi-driver-powerscale/empty_isilon-certs.yaml" ]; then
                    kubectl apply -f /opt/omnia/csi-driver-powerscale/empty_isilon-certs.yaml || {
                      echo "Failed to apply empty certs secret. Continuing..."
                      POWERSCALE_DEPLOYMENT_FAILED=1
                    }
                  fi
                fi

                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Deploying External Snapshotter CRDs ====="
                  if [ -d "/opt/omnia/csi-driver-powerscale/csi-powerscale/external-snapshotter/client/config/crd" ]; then
                    kubectl apply -f /opt/omnia/csi-driver-powerscale/csi-powerscale/external-snapshotter/client/config/crd/ >/dev/null 2>&1 || {
                      echo "CRD deployment failed (expected). Continuing..."
                    }
                  fi
                fi

                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Deploying Snapshot Controller ====="
                  if [ -d "/opt/omnia/csi-driver-powerscale/csi-powerscale/external-snapshotter/deploy/kubernetes/snapshot-controller" ]; then
                    kubectl apply -f /opt/omnia/csi-driver-powerscale/csi-powerscale/external-snapshotter/deploy/kubernetes/snapshot-controller/ >/dev/null 2>&1 || {
                      echo "Snapshot Controller deployment failed (expected). Continuing..."
                    }

                    echo "Updating Snapshot Controller image to v8.3.0..."
                    kubectl set image deployment/snapshot-controller snapshot-controller=registry.k8s.io/sig-storage/snapshot-controller:v8.3.0 -n kube-system >/dev/null 2>&1 || true

                    echo "Waiting for Snapshot Controller rollout to finish (timeout: 5 minutes)..."
                    kubectl rollout status deployment/snapshot-controller -n kube-system --timeout=300s >/dev/null 2>&1 || {
                      echo "Snapshot Controller rollout did not complete in time."
                    }
                    sleep 10
                    echo "Waiting for Snapshot Controller pods to reach Running state..."
                    MAX_ATTEMPTS=60
                    WAIT_TIME=5
                    for ((i=1; i<=MAX_ATTEMPTS; i++)); do
                      not_ready=$(kubectl get pods -n kube-system --no-headers 2>/dev/null | grep snapshot-controller | awk '{ print $3 }' | grep -vE '^(Running|Completed)$' | wc -l)
                      if [ "$not_ready" -eq 0 ]; then
                        echo "Snapshot Controller pods are Running or Completed."
                        break
                      else
                        echo "[$i/$MAX_ATTEMPTS] $not_ready Snapshot Controller pods not ready, waiting ${WAIT_TIME}s..."
                        sleep $WAIT_TIME
                      fi
                    done
                    echo "Snapshot Controller deployment completed (or timed out safely)."
                  fi
                fi

                FILE="/opt/omnia/csi-driver-powerscale/values.yaml"   # <-- update with actual path

                echo "Updating arrayConnectivityPollRate in: $FILE"

                if grep -Fq -- "--arrayConnectivityPollRate=60" "$FILE"; then
                  echo "Found existing poll rate 60. Updating to 20..."
                  sed -i 's/--arrayConnectivityPollRate=60/--arrayConnectivityPollRate=20/g' "$FILE"
                else
                  echo "No poll rate value 60 found. Nothing to change."
                fi

                echo "Done updating poll rate."


                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Running CSI PowerScale installation script ====="
                  INSTALL_SCRIPT="/opt/omnia/csi-driver-powerscale/csi-powerscale/dell-csi-helm-installer/csi-install.sh"
                  if [ -x "$INSTALL_SCRIPT" ]; then
                    cd "$(dirname "$INSTALL_SCRIPT")" || true
                    ./csi-install.sh --namespace isilon --values /opt/omnia/csi-driver-powerscale/values.yaml &
                    CSI_PID=$!
                    echo "Waiting for CSI install script (PID $CSI_PID) to complete..."
                    wait $CSI_PID
                  else
                    echo "ERROR: CSI install script not found."
                    POWERSCALE_DEPLOYMENT_FAILED=1
                  fi
                fi

                if [ "$POWERSCALE_DEPLOYMENT_FAILED" -eq 0 ]; then
                  echo "===== Waiting for CSI pods ====="
                  MAX_ATTEMPTS=10
                  WAIT_TIME=60
                  CSI_READY=0
                  for ((i=1; i<=MAX_ATTEMPTS; i++)); do
                    if kubectl get pods -n isilon --no-headers 2>/dev/null | grep -q '^isilon-'; then
                      NON_RUNNING=$(kubectl get pods -n isilon --no-headers 2>/dev/null | grep '^isilon-' | grep -v "Running" | wc -l)
                      if [ "$NON_RUNNING" -eq 0 ]; then
                        echo "All CSI pods are running."
                        CSI_READY=1
                        break
                      fi
                    fi
                    echo "Attempt $i/$MAX_ATTEMPTS: Waiting for CSI pods to be Running..."
                    kubectl rollout restart deployment isilon-controller -n isilon
                    kubectl rollout restart daemonset isilon-node -n isilon
                    sleep $WAIT_TIME
                  done

                  if [ "$CSI_READY" -eq 1 ]; then
                    echo "CSI PowerScale driver installed successfully."
                    if [ -f "/opt/omnia/csi-driver-powerscale/ps_storage_class.yml" ]; then
                      kubectl apply -f /opt/omnia/csi-driver-powerscale/ps_storage_class.yml
                      echo "PowerScale StorageClass applied successfully."
                    fi
                  else
                    echo "ERROR: CSI PowerScale driver pods not ready after waiting. Skipping StorageClass creation."
                  fi
                  echo "===== PowerScale CSI deployed successfully â€” updating default StorageClass ====="
                  echo "Checking if StorageClass 'nfs-client' exists..."
                  if kubectl get sc nfs-client >/dev/null 2>&1; then
                    NFS_SC_EXISTS=1
                    echo "nfs-client StorageClass found."
                  else
                    NFS_SC_EXISTS=0
                    echo "nfs-client StorageClass NOT found."
                  fi

                  echo "Checking if StorageClass 'ps01' exists..."
                  if kubectl get sc ps01 >/dev/null 2>&1; then
                    PS01_SC_EXISTS=1
                    echo "ps01 StorageClass found."
                  else
                    PS01_SC_EXISTS=0
                    echo "ps01 StorageClass NOT found."
                  fi

                  # Only proceed if ps01 exists (means CSI is installed correctly)
                  if [ "$PS01_SC_EXISTS" -eq 1 ]; then
                    echo "===== Updating StorageClass defaults ====="

                    # Disable default class on nfs-client
                    if [ "$NFS_SC_EXISTS" -eq 1 ]; then
                      echo "Checking if nfs-client is currently default..."
                      DEFAULT_ANNOT=$(kubectl get sc nfs-client -o jsonpath='{.metadata.annotations.storageclass\.kubernetes\.io/is-default-class}' 2>/dev/null)

                      if [ "$DEFAULT_ANNOT" = "true" ]; then
                        echo "Removing default StorageClass annotation from nfs-client..."
                        kubectl annotate sc nfs-client storageclass.kubernetes.io/is-default-class="false" --overwrite
                      else
                        echo "nfs-client is not default. Skipping."
                      fi
                    fi

                    # Set ps01 as default
                    echo "Setting ps01 as default StorageClass..."
                    kubectl annotate sc ps01 storageclass.kubernetes.io/is-default-class="true" --overwrite

                    echo "===== StorageClass update completed successfully ====="
                  else
                    echo "ps01 StorageClass not found. Cannot update default StorageClass settings."
                  fi
                fi
              fi
            fi
            systemctl restart nfs-client.target
            systemctl restart rpcbind

{% if hostvars['localhost']['idrac_telemetry_support'] or hostvars['localhost']['ldms_support'] %}
            echo "Applying Telemetry Kubernetes deployments"
            /root/telemetry.sh
{% endif %}
            echo "Rollout and Restart coredns"
            kubectl rollout restart deployment coredns -n kube-system
            sleep 30
            echo "Waiting for coredns pods to appear.."
            kubectl wait --for=condition=Ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s
            # Mark initialization complete so all of above is skipped on reboot!
            touch "$MARKER"
            echo "Cloud-Init has completed successfully."
          else
            # SUBSEQUENT BOOT - SKIP INIT
            echo "service_kube_control_plane_first_x86_64 is already part of cluster."
            echo "Cluster already initialized. Performing node reboot procedures."
            # CRI and kubelet already enabled above
            # You can log health status etc if you wish:
            mkdir -p $HOME/.kube /root/.kube
            cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
            chown $(id -u):$(id -g) $HOME/.kube/config
            yes | cp -i /etc/kubernetes/admin.conf /root/.kube/config
            kubectl get nodes -o wide || echo "Cluster not yet fully up"
            kubectl get pods --all-namespaces -o wide || echo "Pods may not be ready yet"
            
            echo "Rollout and Restart coredns"
            kubectl rollout restart deployment coredns -n kube-system
            echo "Waiting for coredns pods to appear.."
            sleep 30
            kubectl wait --for=condition=Ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s
            # Wait for all pods in all namespaces to be ready (status=Running or Completed)
            echo "Waiting for all pods to be Ready (Running/Completed)..."
            while true; do
              not_ready=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | awk '{ print $4 }' | grep -vE '^(Running|Completed)$' | wc -l)
              if [ "${not_ready}" -eq 0 ]; then
                echo "All pods are Running or Completed."
                break
              else
                echo "$not_ready pods not yet ready, waiting 5s ..."
                sleep 5
              fi
            done

            echo "Listing all Kubernetes nodes:"
            kubectl get nodes -o wide
            echo "Listing all Kubernetes pods in all namespaces:"
            kubectl get pods --all-namespaces -o wide
            echo "Cloud-Init finished successfully after the reboot."

          fi
