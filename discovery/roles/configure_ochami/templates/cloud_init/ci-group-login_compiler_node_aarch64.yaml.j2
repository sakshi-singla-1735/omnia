- name: {{ functional_group_name }}
  description: "{{ functional_group_name }} config"
  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]
      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
      disable_root: false

      write_files:
        - path: /usr/local/bin/set-ssh.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi

        - path: /usr/local/bin/install_cuda_toolkit.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/cuda_toolkit_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting CUDA Toolkit installation ====="

            # Check if CUDA toolkit is already installed
            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[INFO] CUDA toolkit already installed (version: ${CUDA_VERSION}). Exiting."
                exit 0
            fi

            echo "[INFO] Mounting NFS runfile directory for CUDA toolkit..."
            mkdir -p /cuda-runfile
            mount -t nfs {{ cloud_init_nfs_path }}/runfile /cuda-runfile

            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount NFS runfile share. Exiting."
                exit 1
            fi

            echo "[INFO] Installing CUDA toolkit..."
            if [ -f "/cuda-runfile/cuda_13.0.2_580.95.05_linux_sbsa.run" ]; then
                # Install only the toolkit component
                bash /cuda-runfile/cuda_13.0.2_580.95.05_linux_sbsa.run --silent --toolkit --toolkitpath=/usr/local/cuda --override

                if [ $? -eq 0 ]; then
                    echo "[SUCCESS] CUDA toolkit installed successfully."

                    # Set up environment variables
                    cat > /etc/profile.d/cuda.sh << 'ENDOFFILE'
            export PATH=/usr/local/cuda/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
            export CUDA_HOME=/usr/local/cuda
            ENDOFFILE

                    # Apply environment variables for current session
                    export PATH=/usr/local/cuda/bin:$PATH
                    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
                    export CUDA_HOME=/usr/local/cuda

                    echo "[INFO] CUDA environment configured"
                else
                    echo "[ERROR] CUDA toolkit installation failed."
                fi
            else
                echo "[ERROR] CUDA toolkit runfile not found in /cuda-runfile/"
            fi

            echo "[INFO] Verifying CUDA toolkit installation..."
            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[SUCCESS] CUDA toolkit verified: version $CUDA_VERSION"
                echo "[INFO] CUDA installation path: $(which nvcc)"
            else
                echo "[ERROR] CUDA toolkit (nvcc) not found after installation."
            fi

            echo "[INFO] Setting up shared CUDA directory for compute nodes..."
            # Create shared directory for compute nodes to mount
            mkdir -p /shared-cuda-toolkit
            # Mount the shared NFS location where compute nodes will access the toolkit
            mount -t nfs {{ cloud_init_nfs_path }}/cuda/ /shared-cuda-toolkit

            echo "[INFO] Copying CUDA toolkit to shared location..."
            # Copy the installed CUDA toolkit to the shared location for compute nodes
            #rsync -av /usr/local/cuda/ /shared-cuda-toolkit/ --exclude='*.a' --exclude='doc/'
            cp -r /usr/local/cuda/* /shared-cuda-toolkit/ 2>/dev/null || true

            echo "[INFO] Cleaning up temporary mounts..."
            umount /cuda-runfile 2>/dev/null
            rmdir /cuda-runfile 2>/dev/null

            echo "===== CUDA Toolkit installation completed ====="

        - path: /usr/local/bin/install_nvhpc_sdk.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/nvhpc_sdk_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting NVIDIA HPC SDK installation ====="

            NVHPC_PKG_NAME="nvhpc_2025_2511_Linux_x86_64_cuda_13.0"
            NVHPC_EXPORT="{{ cloud_init_nfs_path }}/hpc_tools/nvidia_sdk"
            NVHPC_MOUNT="/shared-nvhpc-sdk"
            NVHPC_TARBALL="${NVHPC_MOUNT}/${NVHPC_PKG_NAME}.tar.gz"
            NVHPC_INSTALL_DIR_NFS="${NVHPC_MOUNT}/nvhpc"
            NVHPC_LOCAL_MOUNT="/opt/nvidia/nvhpc"

            mkdir -p "${NVHPC_MOUNT}"
            mount -t nfs "${NVHPC_EXPORT}" "${NVHPC_MOUNT}"

            # 1) If already mounted/installed, skip
            if mountpoint -q "${NVHPC_LOCAL_MOUNT}"; then
                echo "[INFO] ${NVHPC_LOCAL_MOUNT} already mounted. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            if [ -d "${NVHPC_LOCAL_MOUNT}" ]; then
                echo "[INFO] ${NVHPC_LOCAL_MOUNT} directory already present. Assuming NVIDIA HPC SDK is installed. Skipping."
                exit 0
            fi

            # 2) Check CUDA pre-req
            echo "[INFO] Checking if CUDA toolkit is installed (nvcc)..."
            if ! command -v nvcc &>/dev/null; then
                echo "[ERROR] CUDA toolkit (nvcc) is not installed. NVIDIA HPC SDK requires CUDA 13.0. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
            echo "[INFO] Detected CUDA toolkit version: ${CUDA_VERSION}"

            # 3) Mount Slurm SDK export to canonical Slurm path
            echo "[INFO] Mounting NFS export for NVIDIA HPC SDK: ${NVHPC_NFS_EXPORT} -> ${NVHPC_NFS_DIR}"
            mkdir -p "${NVHPC_NFS_DIR}"
            mount -t nfs "${NVHPC_NFS_EXPORT}" "${NVHPC_NFS_DIR}"

            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount ${NVHPC_NFS_EXPORT} on ${NVHPC_NFS_DIR}. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            # 4) Check tarball on NFS
            echo "[INFO] Checking for NVIDIA HPC SDK tarball at ${NVHPC_TARBALL}..."
            if [ ! -f "${NVHPC_TARBALL}" ]; then
                echo "[ERROR] NVIDIA HPC SDK tarball not found at ${NVHPC_TARBALL}. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            # 5) Extract on NFS share itself
            echo "[INFO] Extracting NVIDIA HPC SDK tarball under ${NVHPC_NFS_DIR}..."
            rm -rf "${NVHPC_EXTRACT_DIR}"
            tar -xzf "${NVHPC_TARBALL}" -C "${NVHPC_NFS_DIR}"
            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to extract NVIDIA HPC SDK tarball. Skipping installation."
                exit 0
            fi

            echo "[INFO] Ensuring NVHPC install directory exists on NFS: ${NVHPC_INSTALL_DIR_NFS}"
            mkdir -p "${NVHPC_INSTALL_DIR_NFS}"

            # 6) Run installer with target on NFS
            echo "[INFO] Running NVIDIA HPC SDK installer..."
            cd "${NVHPC_EXTRACT_DIR}" || {
                echo "[ERROR] Failed to cd to extracted NVHPC directory: ${NVHPC_EXTRACT_DIR}"
                exit 0
            }

            NVHPC_SILENT=true \
            NVHPC_INSTALL_DIR="${NVHPC_INSTALL_DIR_NFS}" \
            NVHPC_INSTALL_TYPE=auto \
            NVHPC_DEFAULT_CUDA=13.0 \
            ./install 2>&1 | tee -a "${LOGFILE}"
            RC=${PIPESTATUS[3]}

            if [ ${RC} -ne 0 ]; then
                echo "[ERROR] NVIDIA HPC SDK installer exited with status ${RC}. Skipping further configuration."
                exit 0
            fi

            echo "[SUCCESS] NVIDIA HPC SDK installation on NFS completed."

            # 7) Mount NVHPC from NFS into /opt/nvidia/nvhpc
            echo "[INFO] Setting up local mount ${NVHPC_LOCAL_MOUNT} from NFS..."
            mkdir -p /opt/nvidia
            mkdir -p "${NVHPC_LOCAL_MOUNT}"

            NVHPC_INSTALL_EXPORT="{{ cloud_init_nfs_path }}/hpc_tools/nvidia_sdk/nvhpc"

            if ! grep -q "hpc_tools/nvidia_sdk/nvhpc" /etc/fstab; then
                echo "${NVHPC_INSTALL_EXPORT}  ${NVHPC_LOCAL_MOUNT}  nfs defaults,_netdev 0 0" >> /etc/fstab
            fi

            mount "${NVHPC_LOCAL_MOUNT}"
            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount ${NVHPC_LOCAL_MOUNT}. Check NFS configuration."
                exit 0
            fi

            # 8) Environment file
            echo "[INFO] Creating /etc/profile.d/nvhpc.sh..."
            cat > /etc/profile.d/nvhpc.sh << 'EOF'
            export NVHPC_ROOT=/opt/nvidia/nvhpc
            export PATH=$NVHPC_ROOT/Linux_x86_64/25.11/compilers/bin:$PATH
            export LD_LIBRARY_PATH=$NVHPC_ROOT/Linux_x86_64/25.11/compilers/lib:$LD_LIBRARY_PATH
            export MANPATH=$NVHPC_ROOT/Linux_x86_64/25.11/compilers/man:$MANPATH
            EOF

            chmod 0755 /etc/profile.d/nvhpc.sh
            # Apply for current session
            source /etc/profile.d/nvhpc.sh

            # 9) Verification & logging
            echo "[INFO] Verifying NVIDIA HPC SDK installation..."
            if command -v nvc &>/dev/null; then
                echo "[SUCCESS] nvc is available: $(nvc --version 2>&1 | head -n 1)"
            else
                echo "[WARNING] nvc compiler not found in PATH after NVHPC setup."
            fi

            if command -v nvfortran &>/dev/null; then
                echo "[SUCCESS] nvfortran is available: $(nvfortran --version 2>&1 | head -n 1)"
            else
                echo "[WARNING] nvfortran compiler not found in PATH after NVHPC setup."
            fi

            echo "[INFO] NVIDIA HPC SDK NVCC version:"
            if command -v nvcc &>/dev/null; then
                nvcc --version
            else
                echo "[WARNING] nvcc not found after NVHPC setup (using CUDA toolkit nvcc only)."
            fi

            echo "[SUCCESS] NVIDIA HPC SDK installation and configuration completed."
        
{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '{{ file_mode_600 }}'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - path: /root/ldms_sampler.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/ldms/ldms_sampler.sh.j2') | indent(12) }}
{% endif %}

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /usr/local/bin/check_slurm_controller_status.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/slurm/check_slurm_controller_status.sh.j2') | indent(12) }}

      runcmd:
        - /usr/local/bin/set-ssh.sh
        - /usr/local/bin/install_cuda_toolkit.sh
        - /usr/local/bin/install_nvhpc_sdk.sh
        - groupadd -r {{ slurm_group_name }}
        - useradd -r -g {{ slurm_group_name }} -d {{ home_dir }} -s /sbin/nologin {{ user }}

        - mkdir -p /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm /etc/slurm/epilog.d /etc/munge /var/log/track
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm/epilog.d     /etc/slurm/epilog.d      nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ trackfile_nfs_path }}    /var/log/track       nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /etc/slurm/epilog.d/slurmd.service /usr/lib/systemd/system/
        - /usr/local/bin/check_slurm_controller_status.sh
        - chown -R {{ user }}:{{ slurm_group_name }} /var/log/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/run/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool
        - chown -R {{ user }}:{{ slurm_group_name }} /var/lib/slurm
        - chown -R {{ munge_user }}:{{ munge_group }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm
        - chmod {{ file_mode_400 }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /etc/slurm/epilog.d/
        - mkdir -p /var/spool/slurmd
        - chmod {{ file_mode_755 }} /var/spool/slurmd
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool/slurmd
        - setenforce 0
        - systemctl enable firewalld
        - systemctl start firewalld
        - firewall-cmd --permanent --add-service=ssh
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/udp
        - firewall-cmd --permanent --add-port={{  slurm_conf_dict.SlurmdPort }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SlurmdPort }}/udp
        - firewall-cmd --reload
        - systemctl enable sshd
        - systemctl start sshd
        - systemctl enable munge
        - systemctl start munge
        - systemctl enable slurmd
        - systemctl start slurmd
        - systemctl daemon-reexec
        - systemctl restart sshd

{% if hostvars['localhost']['openldap_support'] %}
        - /usr/local/bin/update_ldap_conf.sh
        - mkdir /ldapcerts
        - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /ldapcerts/* /etc/openldap/certs
        - umount /ldapcerts

        - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
        - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
        - firewall-cmd --reload

        - setenforce 0
        - authselect select sssd with-mkhomedir --force
        - sudo systemctl enable --now oddjobd.service
        - sudo systemctl enable --now sssd
        - setsebool -P authlogin_nsswitch_use_ldap on
        - setsebool -P authlogin_yubikey on
        - sudo systemctl restart sssd
        - systemctl restart sshd

{% endif %}

{% if hostvars['localhost']['ucx_support'] or hostvars['localhost']['openmpi_support'] or hostvars['localhost']['ldms_support'] %}
        # Add NFS entry and mount
        - mkdir -p {{ client_mount_path }}
        - echo "{{ cloud_init_slurm_nfs_path }} {{ client_mount_path }} nfs defaults,_netdev 0 0" >> /etc/fstab
        - mount -a
{% endif %}

{% if hostvars['localhost']['ucx_support'] %}
        # UCX build and install
        - |
          UCX_BIN={{ client_mount_path }}/benchmarks/ucx
          mkdir -p {{ client_mount_path }}/compile/ucx
          mkdir -p {{ client_mount_path }}/benchmarks/ucx
          cd {{ client_mount_path }}/compile/ucx
          wget --no-check-certificate https://{{ hostvars['localhost']['admin_nic_ip'] }}:2225/pulp/content/opt/omnia/offline_repo/cluster/aarch64/{{ hostvars['localhost']['cluster_os_type'] }}/{{ hostvars['localhost']['cluster_os_version'] }}/tarball/ucx/ucx.tar.gz -O ucx.tar.gz
          tar xzf ucx.tar.gz
          cd ucx-*
          mkdir -p build
          cd build
          ../contrib/configure-release --prefix={{ client_mount_path }}/benchmarks/ucx
          make -j 8
          make install
{% endif %}

{% if hostvars['localhost']['openmpi_support'] %}
        # OpenMPI build and install with UCX + Slurm detection
        - |
          OPENMPI_INSTALL_PREFIX="{{ client_mount_path }}/benchmarks/openmpi"
          OPENMPI_SRC="{{ client_mount_path }}/compile/openmpi"
          mkdir -p $OPENMPI_SRC
          mkdir -p $OPENMPI_INSTALL_PREFIX

          cd $OPENMPI_SRC
          wget --no-check-certificate https://{{ hostvars['localhost']['admin_nic_ip'] }}:2225/pulp/content/opt/omnia/offline_repo/cluster/aarch64/{{ hostvars['localhost']['cluster_os_type'] }}/{{ hostvars['localhost']['cluster_os_version'] }}/tarball/openmpi/openmpi.tar.gz -O openmpi.tar.gz

          tar xzf openmpi.tar.gz
          cd openmpi-*
          mkdir -p build

          # Check Slurm
          if sinfo >/dev/null 2>&1; then
            SLURM_FLAG="--with-slurm=yes --with-munge=/usr"
          else
            SLURM_FLAG="--with-slurm=no"
          fi

          # Check UCX
          if [ -x "{{ client_mount_path }}/benchmarks/ucx/bin/ucx_info" ]; then
            {{ client_mount_path }}/benchmarks/ucx/bin/ucx_info -v
            if [ $? -eq 0 ]; then
              UCX_FLAG="--with-ucx={{ client_mount_path }}/benchmarks/ucx"
            else
              echo "ucx_info failed, disabling UCX"
              UCX_FLAG=""
            fi
          else
            echo "ucx_info not found, disabling UCX"
            UCX_FLAG=""
          fi

          cd build
          ../configure --prefix=$OPENMPI_INSTALL_PREFIX \
            --enable-mpi1-compatibility \
            --enable-prte-prefix-by-default \
            $SLURM_FLAG $UCX_FLAG 2>&1 | tee config.out

          make -j 8
          make install
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - echo " Starting LDMS setup " | tee -a /var/log/ldms-cloudinit.log

        - /root/ldms_sampler.sh
{% endif %}
        - echo "Cloud-Init has completed successfully."
