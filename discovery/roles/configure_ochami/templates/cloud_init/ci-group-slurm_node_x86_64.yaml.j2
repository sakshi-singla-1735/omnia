- name: {{ functional_group_name }}
  description: "{{ functional_group_name }}"

  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]

      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
      disable_root: false

      write_files:
        - path: /usr/local/bin/set-hostname-by-mac.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            # Autogenerated hostname & IP setter based on NIC MAC
            DOMAIN_NAME={{ hostvars['localhost']['domain_name'] }}
            for IFACE in /sys/class/net/*; do
                MAC=$(cat $IFACE/address | tr '[:upper:]' '[:lower:]')
                case $MAC in
                {% for node in nodes %}
                {{ node.interfaces[0].mac_addr | lower }}) HOST={{ node.name }} IP={{ node.interfaces[0].ip_addrs[0].ip_addr }} ;;
                {% endfor %}
                esac
            done

            if [ -n "$HOST" ]; then
                if ! grep -q "$HOST" /etc/hosts; then
                    echo "$IP $HOST.$DOMAIN_NAME" >> /etc/hosts
                fi
                hostnamectl set-hostname "$HOST.$DOMAIN_NAME"
                sysctl kernel.hostname=$HOST.$DOMAIN_NAME
            fi

            echo 'root:{{ hostvars['localhost']['provision_password'] }}' | chpasswd
            timedatectl set-timezone {{ hostvars['localhost']['timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd

{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '0600'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /usr/local/bin/wait_for_ip.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/wait_for_ip.log"
            IP="$1"
            TIMEOUT={{ ip_timeout }}
            MAX_RETRIES={{ ip_wait_loop }}
            COUNT=0
            echo "Waiting for $IP to become reachable..." | tee -a "$LOGFILE"
            if [[ $# -eq 0 || -z "${1:-}" ]]; then
              echo "Usage: $0 <TARGET_IP>" | tee -a "$LOGFILE"
              echo "No argument provided. continuing" | tee -a "$LOGFILE"
              exit 0
            fi
            while true; do
              if ping -c1 -W1 "$IP" &>/dev/null; then
                  echo "$IP is up â€” proceeding." | tee -a "$LOGFILE"
                  exit 0
              fi
              ((COUNT++))
              if [ "$COUNT" -ge "$MAX_RETRIES" ]; then
                echo "IP $IP not reachable after $MAX_RETRIES attempts, giving up." | tee -a "$LOGFILE"
                exit 1
              fi
              echo "IP $IP not reachable yet (attempt $COUNT). Retrying in $TIMEOUT seconds..." | tee -a "$LOGFILE"
              sleep "$TIMEOUT"
            done


        - path: /root/install_nvidia_driver.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/nvidia_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting NVIDIA GPU detection and driver installation ====="

            # Check for NVIDIA GPU presence
            echo "[INFO] Checking for NVIDIA GPU..."
            if ! lspci | grep -i nvidia &>/dev/null; then
                echo "[INFO] No NVIDIA GPU detected. Exiting."
                exit 0
            fi

            echo "[INFO] NVIDIA GPU detected. Proceeding with setup."

            # Check if NVIDIA driver is already installed
            if command -v nvidia-smi &>/dev/null; then
                echo "[INFO] NVIDIA driver already installed. Skipping driver installation."
                DRIVER_INSTALLED=true
            else
                DRIVER_INSTALLED=false
            fi

            # Function to mount NFS and setup CUDA environment
            setup_cuda_environment() {
                local nfs_share="182.168.0.34:/nfs-server/slurm/xslurmnode/etc/slurm/epilog.d/cuda/cuda-libs"
                local mount_point="/mnt/nfs-cuda"

                echo "[INFO] Mounting NFS share for CUDA toolkit: $nfs_share"
                mkdir -p $mount_point
                mount -t nfs $nfs_share $mount_point

                if [ $? -ne 0 ]; then
                    echo "[WARNING] Failed to mount NFS share for CUDA toolkit: $nfs_share"
                    return 1
                fi

                if [ -x "$mount_point/bin/nvcc" ]; then
                    echo "[INFO] Found shared CUDA installation on NFS: $mount_point"

                    ln -sf $mount_point /usr/local/cuda 2>/dev/null

                    cat > /etc/profile.d/cuda.sh << 'ENDOFFILE'
            export PATH=$mount_point/bin:\$PATH
            export LD_LIBRARY_PATH=$mount_point/lib64:\$LD_LIBRARY_PATH
            export CUDA_HOME=$mount_point
            ENDOFFILE

                    export PATH=$mount_point/bin:$PATH
                    export LD_LIBRARY_PATH=$mount_point/lib64:$LD_LIBRARY_PATH
                    export CUDA_HOME=$mount_point

                    mkdir -p /etc/slurm/epilog.d/cuda
                    ln -sf $mount_point /etc/slurm/epilog.d/cuda/cuda-libs 2>/dev/null

                    echo "[SUCCESS] CUDA environment configured from NFS share"
                    return 0
                else
                    echo "[WARNING] CUDA toolkit not found in mounted NFS at $mount_point"
                    echo "[DEBUG] Checking mount point contents:"
                    ls -la $mount_point/ 2>/dev/null || echo "Cannot list directory"
                    umount $mount_point 2>/dev/null
                    return 1
                fi
            }

            # Check if CUDA toolkit is already available
            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[INFO] CUDA toolkit already available (version: ${CUDA_VERSION})."
                TOOLKIT_AVAILABLE=true
            else
                if setup_cuda_environment; then
                    echo "[INFO] Successfully configured CUDA environment from NFS share."
                    TOOLKIT_AVAILABLE=true
                else
                    echo "[INFO] CUDA toolkit not available locally or via NFS."
                    TOOLKIT_AVAILABLE=false
                fi
            fi
            if [ "$DRIVER_INSTALLED" = true ] && [ "$TOOLKIT_AVAILABLE" = true ]; then
                echo "[INFO] NVIDIA driver and CUDA toolkit are already available. Nothing to do."
                echo "===== NVIDIA GPU setup completed ====="
                exit 0
            fi

            echo "[INFO] Mounting NFS runfile directory..."
            mkdir -p /gpu-runfile
            mount -t nfs 182.168.0.34:/nfs-server/slurm/$(hostname -s)/etc/slurm/epilog.d/runfile /gpu-runfile

            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount NFS runfile share. Exiting."
                exit 1
            fi

            if [ "$DRIVER_INSTALLED" = false ]; then
                echo "[INFO] Installing NVIDIA driver..."
                if [ -f "/gpu-runfile/cuda_13.0.2_580.95.05_linux.run" ]; then
                    bash /gpu-runfile/cuda_13.0.2_580.95.05_linux.run --silent --driver --no-opengl-libs --kernel-source-path=/lib/modules/$(uname -r)/build
                    if [ $? -eq 0 ] && command -v nvidia-smi &>/dev/null; then
                        echo "[SUCCESS] NVIDIA driver installed successfully."
                    else
                        echo "[ERROR] NVIDIA driver installation failed."
                    fi
                else
                    echo "[ERROR] NVIDIA driver runfile not found in /gpu-runfile/"
                fi
            fi

            if [ "$TOOLKIT_AVAILABLE" = false ]; then
                echo "[INFO] Setting up CUDA toolkit..."
                if ! command -v nvcc &>/dev/null; then
                    if setup_cuda_environment; then
                        echo "[SUCCESS] Using shared CUDA installation from NFS."
                    else
                        echo "[INFO] Shared CUDA not available, installing locally..."
                        if [ -f "/gpu-runfile/cuda_13.0.2_580.95.05_linux.run" ]; then
                            bash /gpu-runfile/cuda_13.0.2_580.95.05_linux.run --silent --toolkit --toolkitpath=/etc/slurm/epilog.d/cuda/cuda-libs --override

                            export PATH=/etc/slurm/epilog.d/cuda/cuda-libs/bin:$PATH
                            export LD_LIBRARY_PATH=/etc/slurm/epilog.d/cuda/cuda-libs/lib64:$LD_LIBRARY_PATH

                            cat > /etc/profile.d/cuda.sh << 'ENDOFFILE'
            export PATH=/etc/slurm/epilog.d/cuda/cuda-libs/bin:\$PATH
            export LD_LIBRARY_PATH=/etc/slurm/epilog.d/cuda/cuda-libs/lib64:\$LD_LIBRARY_PATH
            export CUDA_HOME=/etc/slurm/epilog.d/cuda/cuda-libs
            ENDOFFILE

                            echo "[INFO] Local CUDA toolkit installation completed."
                        else
                            echo "[ERROR] CUDA toolkit runfile not found in /gpu-runfile/"
                        fi
                    fi
                fi
            fi

            echo "[INFO] Verifying installations..."
            if command -v nvidia-smi &>/dev/null; then
                nvidia_version=$(nvidia-smi --version | head -n1)
                echo "[SUCCESS] NVIDIA driver verified: $nvidia_version"
            else
                echo "[ERROR] NVIDIA driver not found after installation."
            fi

            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[SUCCESS] CUDA toolkit verified: version $CUDA_VERSION"
                echo "[INFO] CUDA installation path: $(which nvcc)"
            else
                echo "[WARNING] CUDA toolkit (nvcc) not found in PATH."
            fi

            echo "[INFO] Cleaning up NFS mounts..."
            #umount /gpu-runfile 2>/dev/null
            #umount /mnt/nfs-cuda 2>/dev/null
            #rmdir /gpu-runfile /mnt/nfs-cuda 2>/dev/null

            echo "===== NVIDIA GPU setup completed ====="

      runcmd:
        - /usr/local/bin/set-hostname-by-mac.sh
        - echo 'export PATH=/etc/slurm/epilog.d/cuda/cuda-libs/bin:/usr/local/cuda/bin:$PATH' > /etc/profile.d/cuda_path.sh
        - /root/install_nvidia_driver.sh
        - mkdir -p /var/log
        - /usr/local/bin/wait_for_ip.sh {{ ctld_list[0] }}
        - |
          if [ $? -ne 0 ]; then
            echo "IP reachability of slurm control node failed, stopping further execution."
            exit 0
          fi
        - useradd -mG wheel -p '$6$VHdSKZNm$O3iFYmRiaFQCemQJjhfrpqqV7DdHBi5YpY6Aq06JSQpABPw.3d8PQ8bNY9NuZSmDv7IL/TsrhRJ6btkgKaonT.' testuser
        - groupadd -r {{ slurm_group_name }}
        - useradd -r -g {{ slurm_group_name }} -d {{ home_dir }} -s /sbin/nologin {{ user }}
        - mkdir -pv /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm /etc/slurm/epilog.d /etc/munge
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm/epilog.d     /etc/slurm/epilog.d      nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -av
        - yes | cp /etc/slurm/epilog.d/slurmd.service /usr/lib/systemd/system/
        - chown -R {{ user }}:{{ slurm_group_name }} /var/log/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/run/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool
        - chown -R {{ user }}:{{ slurm_group_name }} /var/lib/slurm
        - chown -R {{ munge_user }}:{{ munge_group }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm
        - chmod {{ file_mode_400 }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /etc/slurm/epilog.d/
        - chmod +x /etc/slurm/epilog.d/logout_user.sh
        - mkdir -p /var/spool/slurmd
        - chmod {{ file_mode_755 }} /var/spool/slurmd
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool/slurmd
        - sed -i '/^password\s\+include\s\+password-auth/i account    required    pam_slurm_adopt.so action_no_jobs=deny' /etc/pam.d/sshd
        - setenforce 0
        - systemctl enable firewalld
        - systemctl start firewalld
        - firewall-cmd --permanent --add-service=ssh
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/udp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SlurmdPort }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SlurmdPort }}/udp
        - firewall-cmd --reload
        - systemctl enable sshd
        - systemctl start sshd
        - systemctl enable munge
        - systemctl start munge
        - systemctl enable slurmd
        - systemctl start slurmd
        - systemctl daemon-reexec
        - systemctl restart sshd
        - scontrol reconfigure

{% if hostvars['localhost']['openldap_support'] %}
        - /usr/local/bin/update_ldap_conf.sh
        - mkdir /ldapcerts
        - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /ldapcerts/* /etc/openldap/certs
        - umount /ldapcerts
        - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
        - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
        - firewall-cmd --reload
        - setenforce 0
        - authselect select sssd with-mkhomedir --force
        - sudo systemctl enable --now oddjobd.service
        - sudo systemctl enable --now sssd
        - setsebool -P authlogin_nsswitch_use_ldap on
        - setsebool -P authlogin_yubikey on
        - sudo systemctl restart sssd
        - systemctl restart sshd
{% endif %}
