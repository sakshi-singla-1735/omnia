- name: {{ functional_group_name }}
  description: "{{ functional_group_name }} config"
  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]
      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
      disable_root: false

      write_files:
        - path: /usr/local/bin/set-ssh.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi


        - path: /usr/local/bin/install_cuda_toolkit.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/cuda_toolkit_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting CUDA Toolkit installation ====="

            # Check if CUDA toolkit is already installed
            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[INFO] CUDA toolkit already installed (version: ${CUDA_VERSION}). Exiting."
                exit 0
            fi

            echo "[INFO] Mounting NFS runfile directory for CUDA toolkit..."
            mkdir -p /cuda-runfile
            mount -t nfs {{ cloud_init_nfs_path }}/runfile /cuda-runfile

            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount NFS runfile share. Exiting."
                exit 1
            fi

            echo "[INFO] Installing CUDA toolkit..."
            if [ -f "/cuda-runfile/cuda_13.0.2_580.95.05_linux.run" ]; then
                # Install only the toolkit component
                bash /cuda-runfile/cuda_13.0.2_580.95.05_linux.run --silent --toolkit --toolkitpath=/usr/local/cuda --override

                if [ $? -eq 0 ]; then
                    echo "[SUCCESS] CUDA toolkit installed successfully."

                    # Set up environment variables
                    cat > /etc/profile.d/cuda.sh << 'ENDOFFILE'
            export PATH=/usr/local/cuda/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
            export CUDA_HOME=/usr/local/cuda
            ENDOFFILE

                    # Apply environment variables for current session
                    export PATH=/usr/local/cuda/bin:$PATH
                    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
                    export CUDA_HOME=/usr/local/cuda

                    echo "[INFO] CUDA environment configured"
                else
                    echo "[ERROR] CUDA toolkit installation failed."
                fi
            else
                echo "[ERROR] CUDA toolkit runfile not found in /cuda-runfile/"
            fi

            echo "[INFO] Verifying CUDA toolkit installation..."
            if command -v nvcc &>/dev/null; then
                CUDA_VERSION=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[SUCCESS] CUDA toolkit verified: version $CUDA_VERSION"
                echo "[INFO] CUDA installation path: $(which nvcc)"
            else
                echo "[ERROR] CUDA toolkit (nvcc) not found after installation."
            fi

            echo "[INFO] Setting up shared CUDA directory for compute nodes..."
            # Create shared directory for compute nodes to mount
            mkdir -p /shared-cuda-toolkit
            # Mount the shared NFS location where compute nodes will access the toolkit
            mount -t nfs {{ cloud_init_nfs_path }}/cuda/ /shared-cuda-toolkit

            echo "[INFO] Copying CUDA toolkit to shared location..."
            # Copy the installed CUDA toolkit to the shared location for compute nodes
            #rsync -av /usr/local/cuda/ /shared-cuda-toolkit/ --exclude='*.a' --exclude='doc/'
            cp -r /usr/local/cuda/* /shared-cuda-toolkit/ 2>/dev/null || true

            echo "[INFO] Cleaning up temporary mounts..."
            umount /cuda-runfile 2>/dev/null
            rmdir /cuda-runfile 2>/dev/null

            echo "===== CUDA Toolkit installation completed ====="

        - path: /usr/local/bin/install_nvhpc_sdk.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/nvhpc_sdk_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting NVIDIA HPC SDK installation ====="

            NVHPC_PKG_NAME="nvhpc_2025_2511_Linux_x86_64_cuda_13.0"
            NVHPC_EXPORT="{{ cloud_init_nfs_path }}/hpc_tools/nvidia_sdk"
            NVHPC_MOUNT="/shared-nvhpc-sdk"
            NVHPC_TARBALL="${NVHPC_MOUNT}/${NVHPC_PKG_NAME}.tar.gz"
            NVHPC_INSTALL_DIR_NFS="${NVHPC_MOUNT}/nvhpc"
            NVHPC_LOCAL_MOUNT="/opt/nvidia/nvhpc"
            NVHPC_EXTRACT_DIR="${NVHPC_MOUNT}/${NVHPC_PKG_NAME}"

            # If already mounted/installed, skip otherwise mount
            if mountpoint -q "${NVHPC_LOCAL_MOUNT}"; then
                echo "[INFO] ${NVHPC_LOCAL_MOUNT} already mounted. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            if [ -d "${NVHPC_LOCAL_MOUNT}" ]; then
                echo "[INFO] ${NVHPC_LOCAL_MOUNT} directory already present. Assuming NVIDIA HPC SDK is installed. Skipping."
                exit 0
            fi

            mkdir -p "${NVHPC_MOUNT}"
            mount -t nfs "${NVHPC_EXPORT}" "${NVHPC_MOUNT}"
            if [ $? -ne 0 ]; then
                echo "[ERROR] Failed to mount ${NVHPC_EXPORT} on ${NVHPC_MOUNT}. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            # Check tarball on NFS
            echo "[INFO] Checking for NVIDIA HPC SDK tarball at ${NVHPC_TARBALL}..."
            if [ ! -f "${NVHPC_TARBALL}" ]; then
                echo "[ERROR] NVIDIA HPC SDK tarball not found at ${NVHPC_TARBALL}. Skipping NVIDIA HPC SDK installation."
                exit 0
            fi

            # 5) Extract on NFS share itself
            EXTRACT_SIZE_GB=$(du -sBG "${NVHPC_EXTRACT_DIR}" 2>/dev/null | cut -f1 | tr -d 'G')

            if [ -d "${NVHPC_EXTRACT_DIR}" ] && [ "${EXTRACT_SIZE_GB}" -ge 13 ] && [ -f "${NVHPC_EXTRACT_DIR}/install" ]; then
                echo "[INFO] NVHPC already extracted (size=${EXTRACT_SIZE_GB}G, install file exists). Skipping extraction."
            else
                echo "[INFO] Extracting NVIDIA HPC SDK tarball under ${NVHPC_MOUNT}..."
                # Optional: SHOw checkpoint progress
                tar -xzf "${NVHPC_TARBALL}" -C "${NVHPC_MOUNT}" \
                --checkpoint=2000 \
                --checkpoint-action=echo="[INFO] Extracting NVHPC... please wait"
    
                if [ $? -ne 0 ]; then
                    echo "[ERROR] Failed to extract NVIDIA HPC SDK tarball. Skipping installation."
                    exit 0
                fi
            fi
            
            echo "[INFO] Ensuring NVHPC install directory exists on NFS: ${NVHPC_INSTALL_DIR_NFS}"
            mkdir -p "${NVHPC_INSTALL_DIR_NFS}"

            # Run installer with target on NFS
            INSTALL_BIN_DIR="${NVHPC_INSTALL_DIR_NFS}/Linux_x86_64/25.11/compilers/bin"

            if [ -x "${INSTALL_BIN_DIR}/nvc" ]; then
                echo "[INFO] NVHPC already installed at ${NVHPC_INSTALL_DIR_NFS} (nvc found). Skipping installer."
            else
                echo "[INFO] Running NVIDIA HPC SDK installer..."
                cd "${NVHPC_EXTRACT_DIR}" || {
                    echo "[ERROR] Failed to cd to extracted NVHPC directory: ${NVHPC_EXTRACT_DIR}"
                    exit 0
                }

                NVHPC_SILENT=true \
                NVHPC_INSTALL_DIR="${NVHPC_INSTALL_DIR_NFS}" \
                NVHPC_INSTALL_TYPE=auto \
                ./install 2>&1 | tee -a "${LOGFILE}"

                RC=${PIPESTATUS[0]}
                echo "[INFO] NVHPC installer exited with return code: ${RC}"

                if [ ${RC} -ne 0 ]; then
                    echo "[ERROR] NVIDIA HPC SDK installer failed with status ${RC}. Skipping further configuration."
                    exit 0
                fi
            fi

            echo "[SUCCESS] NVIDIA HPC SDK installation on NFS completed."
            
            # Mount NVHPC from NFS into /opt/nvidia/nvhpc
            echo "[INFO] Setting up local NVHPC mount at ${NVHPC_LOCAL_MOUNT}..."

            mkdir -p "${NVHPC_LOCAL_MOUNT}"

            NVHPC_INSTALL_EXPORT="{{ cloud_init_nfs_path }}/hpc_tools/nvidia_sdk/nvhpc"
            FSTAB_ENTRY="${NVHPC_INSTALL_EXPORT} ${NVHPC_LOCAL_MOUNT} nfs defaults,_netdev 0 0"

            # Add fstab entry only if it does not already exist
            if ! grep -qE "^[^#].*${NVHPC_INSTALL_EXPORT}[[:space:]]+${NVHPC_LOCAL_MOUNT}[[:space:]]+nfs" /etc/fstab; then
                echo "[INFO] Adding NVHPC mount to /etc/fstab"
                echo "${FSTAB_ENTRY}" >> /etc/fstab
            else
                echo "[INFO] NVHPC mount already present in /etc/fstab"
            fi

            # Mount using fstab entry
            echo "[INFO] Mounting ${NVHPC_LOCAL_MOUNT}..."
            if ! mount "${NVHPC_LOCAL_MOUNT}"; then
                echo "[ERROR] Failed to mount ${NVHPC_LOCAL_MOUNT}. Check NFS export and /etc/fstab."
                exit 0
            fi

            cat > /etc/profile.d/nvhpc.sh << 'EOF'
        export NVHPC_ROOT=/shared-nvhpc-sdk/nvhpc
        export PATH=$NVHPC_ROOT/Linux_x86_64/25.11/compilers/bin:$PATH
        export MANPATH=$NVHPC_ROOT/Linux_x86_64/25.11/compilers/man:$MANPATH
        EOF

            chmod 644 /etc/profile.d/nvhpc.sh

            echo "[INFO] NVHPC successfully mounted at ${NVHPC_LOCAL_MOUNT}"
        
{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '{{ file_mode_600 }}'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - path: /root/ldms_sampler.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/ldms/ldms_sampler.sh.j2') | indent(12) }}
{% endif %}

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /usr/local/bin/check_slurm_controller_status.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/slurm/check_slurm_controller_status.sh.j2') | indent(12) }}

      runcmd:
        - /usr/local/bin/set-ssh.sh
        - /usr/local/bin/install_cuda_toolkit.sh
        - groupadd -r {{ slurm_group_name }}
        - useradd -r -g {{ slurm_group_name }} -d {{ home_dir }} -s /sbin/nologin {{ user }}

        - mkdir -p /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm /etc/slurm/epilog.d /etc/munge /cert /var/log/track
        - echo "{{ cloud_init_nfs_path }}/cert  /cert   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm/epilog.d     /etc/slurm/epilog.d      nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ trackfile_nfs_path }}    /var/log/track       nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /etc/slurm/epilog.d/slurmd.service /usr/lib/systemd/system/
        - /usr/local/bin/check_slurm_controller_status.sh
        - chown -R {{ user }}:{{ slurm_group_name }} /var/log/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/run/slurm
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool
        - chown -R {{ user }}:{{ slurm_group_name }} /var/lib/slurm
        - chown -R {{ munge_user }}:{{ munge_group }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm
        - chmod {{ file_mode_400 }} /etc/munge/munge.key
        - chmod {{ file_mode_755 }} /etc/slurm/epilog.d/
        - mkdir -p /var/spool/slurmd
        - chmod {{ file_mode_755 }} /var/spool/slurmd
        - chown -R {{ user }}:{{ slurm_group_name }} /var/spool/slurmd
        - setenforce 0
        - systemctl enable firewalld
        - systemctl start firewalld
        - firewall-cmd --permanent --add-service=ssh
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SrunPortRange }}/udp
        - firewall-cmd --permanent --add-port={{  slurm_conf_dict.SlurmdPort }}/tcp
        - firewall-cmd --permanent --add-port={{ slurm_conf_dict.SlurmdPort }}/udp
        - firewall-cmd --reload
        - systemctl enable sshd
        - systemctl start sshd
        - systemctl enable munge
        - systemctl start munge
        - systemctl enable slurmd
        - systemctl start slurmd
        - systemctl daemon-reexec
        - systemctl restart sshd
        - cp /cert/pulp_webserver.crt /etc/pki/ca-trust/source/anchors && update-ca-trust
        - sed -i 's/^gpgcheck=1/gpgcheck=0/' /etc/dnf/dnf.conf
        # nvidia sdk install 
        - /usr/local/bin/install_nvhpc_sdk.sh


{% if hostvars['localhost']['openldap_support'] %}
        - /usr/local/bin/update_ldap_conf.sh
        - mkdir /ldapcerts
        - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /ldapcerts/* /etc/openldap/certs
        - umount /ldapcerts

        - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
        - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
        - firewall-cmd --reload

        - setenforce 0
        - authselect select sssd with-mkhomedir --force
        - sudo systemctl enable --now oddjobd.service
        - sudo systemctl enable --now sssd
        - setsebool -P authlogin_nsswitch_use_ldap on
        - setsebool -P authlogin_yubikey on
        - sudo systemctl restart sssd
        - systemctl restart sshd

{% endif %}

{% if hostvars['localhost']['ucx_support'] or hostvars['localhost']['openmpi_support'] or hostvars['localhost']['ldms_support'] %}
        # Add NFS entry and mount
        - mkdir -p {{ client_mount_path }}
        - echo "{{ cloud_init_slurm_nfs_path }} {{ client_mount_path }} nfs defaults,_netdev 0 0" >> /etc/fstab
        - mount -a
{% endif %}

{% if hostvars['localhost']['ucx_support'] %}
        # UCX build and install
        - |
          UCX_BIN={{ client_mount_path }}/benchmarks/ucx
          mkdir -p {{ client_mount_path }}/compile/ucx
          mkdir -p {{ client_mount_path }}/benchmarks/ucx
          cd {{ client_mount_path }}/compile/ucx
          wget --no-check-certificate https://{{ hostvars['localhost']['admin_nic_ip'] }}:2225/pulp/content/opt/omnia/offline_repo/cluster/x86_64/{{ hostvars['localhost']['cluster_os_type'] }}/{{ hostvars['localhost']['cluster_os_version'] }}/tarball/ucx/ucx.tar.gz -O ucx.tar.gz
          tar xzf ucx.tar.gz
          cd ucx-*
          mkdir -p build
          cd build
          ../contrib/configure-release --prefix={{ client_mount_path }}/benchmarks/ucx
          make -j 8
          make install
{% endif %}

{% if hostvars['localhost']['openmpi_support'] %}
        # OpenMPI build and install with UCX + Slurm detection
        - |
          OPENMPI_INSTALL_PREFIX="{{ client_mount_path }}/benchmarks/openmpi"
          OPENMPI_SRC="{{ client_mount_path }}/compile/openmpi"
          mkdir -p $OPENMPI_SRC
          mkdir -p $OPENMPI_INSTALL_PREFIX

          cd $OPENMPI_SRC
          wget --no-check-certificate https://{{ hostvars['localhost']['admin_nic_ip'] }}:2225/pulp/content/opt/omnia/offline_repo/cluster/x86_64/{{ hostvars['localhost']['cluster_os_type'] }}/{{ hostvars['localhost']['cluster_os_version'] }}/tarball/openmpi/openmpi.tar.gz -O openmpi.tar.gz

          tar xzf openmpi.tar.gz
          cd openmpi-*
          mkdir -p build

          # Check Slurm
          if sinfo >/dev/null 2>&1; then
            SLURM_FLAG="--with-slurm=yes --with-munge=/usr"
          else
            SLURM_FLAG="--with-slurm=no"
          fi

          # Check UCX
          if [ -x "{{ client_mount_path }}/benchmarks/ucx/bin/ucx_info" ]; then
            {{ client_mount_path }}/benchmarks/ucx/bin/ucx_info -v
            if [ $? -eq 0 ]; then
              UCX_FLAG="--with-ucx={{ client_mount_path }}/benchmarks/ucx"
            else
              echo "ucx_info failed, disabling UCX"
              UCX_FLAG=""
            fi
          else
            echo "ucx_info not found, disabling UCX"
            UCX_FLAG=""
          fi

          cd build
          ../configure --prefix=$OPENMPI_INSTALL_PREFIX \
            --enable-mpi1-compatibility \
            --enable-prte-prefix-by-default \
            $SLURM_FLAG $UCX_FLAG 2>&1 | tee config.out

          make -j 8
          make install
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - echo " Starting LDMS setup " | tee -a /var/log/ldms-cloudinit.log

        - /root/ldms_sampler.sh
{% endif %}
        - echo "Cloud-Init has completed successfully."
