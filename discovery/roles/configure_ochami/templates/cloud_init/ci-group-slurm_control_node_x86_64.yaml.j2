- name: {{ functional_group_name }}
  description: "{{ functional_group_name }}"

  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]

      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
        - name: {{ slurm_user }}
          uid: {{ slurm_uid }}
          system: true
          no_create_home: true
          shell: /sbin/nologin
      disable_root: false

      write_files:
        - path: /usr/local/bin/doca-install.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/doca-install.sh.j2') | indent(12) }}

        - path: /usr/local/bin/configure-ib-network.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/configure-ib-network.sh.j2') | indent(12) }}

        - path: /usr/local/bin/set-ssh.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi

        - path: /root/.ssh/config
          permissions: '0600'
          content: |
            Host {{ slurm_control_ssh_patterns }}
                IdentityFile {{ client_mount_path }}/slurm/ssh/oim_rsa
                IdentitiesOnly yes

{% if powervault_config is defined %}
        - path: /usr/local/bin/setup_iscsi_storage.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            set -euo pipefail

            LOGFILE="/var/log/omnia_iscsi_setup.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*"; }

            PORTALS=({% for ip in powervault_config.ip %}"{{ ip }}" {% endfor %})
            PORT="{{ powervault_config.port | default(3260) }}"
            INITIATOR_IQN="{{ powervault_config.isci_initiators | default('') }}"
            VOLUME_ID="{{ powervault_config.volume_id | default('') }}"
            FS_TYPE="{{ powervault_config.fs_type | default('xfs') }}"
            MOUNT_OPTS="{{ powervault_config.mount_options | default('defaults,_netdev,noatime') }}"

            PERSIST_MOUNT="/mnt/slurm-persist"
            MYSQL_SUBDIR="${PERSIST_MOUNT}/mysql"
            SPOOL_SUBDIR="${PERSIST_MOUNT}/spool"

            log "Enabling iSCSI daemon"
            systemctl enable --now iscsid
            /sbin/mpathconf --enable || true

            if [[ -n "${INITIATOR_IQN}" ]]; then
              log "Setting InitiatorName to ${INITIATOR_IQN}"
              if [[ -f /etc/iscsi/initiatorname.iscsi ]] && grep -q "^InitiatorName=${INITIATOR_IQN}$" /etc/iscsi/initiatorname.iscsi; then
                log "InitiatorName already set; not changing"
              else
                printf "InitiatorName=%s\n" "${INITIATOR_IQN}" > /etc/iscsi/initiatorname.iscsi
                log "Restarting iscsid after InitiatorName change"
                systemctl restart iscsid
              fi
            else
              log "INITIATOR_IQN not set; leaving /etc/iscsi/initiatorname.iscsi unchanged"
            fi

            log "Current initiatorname:"
            cat /etc/iscsi/initiatorname.iscsi || true

            log "Discovering iSCSI targets from all portals"
            TARGET_IQN=""

            for ip in "${PORTALS[@]}"; do
              log "Trying discovery on ${ip}:${PORT}"
              OUT=$(iscsiadm -m discovery -t sendtargets -p "${ip}:${PORT}" 2>/dev/null || true)
              echo "$OUT"
              if [[ -z "${TARGET_IQN}" ]]; then
                CANDIDATE_IQN=$(echo "$OUT" | awk '{print $2}' | head -1)
                if [[ -n "${CANDIDATE_IQN}" ]]; then
                  TARGET_IQN="${CANDIDATE_IQN}"
                fi
              fi
            done

            if [[ -z "${TARGET_IQN}" ]]; then
              log "ERROR: Unable to determine target IQN from discovery output"
              exit 1
            fi
            log "Discovered TARGET_IQN=${TARGET_IQN}"

            log "Logging in to ALL discovered iSCSI targets"
            iscsiadm -m node --login || true

            log "Setting automatic startup for all nodes"
            iscsiadm -m node --op update -n node.startup -v automatic || true

            log "Waiting for devices to settle..."
            sleep 5

            log "Enabling multipathd"
            systemctl enable --now multipathd || true

            log "Rescanning iSCSI sessions"
            iscsiadm -m session --rescan || true

            log "Reloading multipath configuration"
            multipath -r || true

            sleep 3

            log "Verifying disks"
            lsblk || true
            lsscsi -s 2>/dev/null | grep -iE "ME|DELL" || true

            log "Multipath devices:"
            multipath -ll || true

            LATEST_MPATH=""

            if [[ -n "${VOLUME_ID}" ]]; then
              log "Selecting multipath using VOLUME_ID match: ${VOLUME_ID}"
              LATEST_MPATH=$(multipath -ll 2>/dev/null | grep -iF "${VOLUME_ID}" | awk '{print $1}' | head -1 || true)
            fi

            if [[ -z "${LATEST_MPATH}" ]]; then
              log "Selecting multipath using vendor match DellEMC,ME5"
              LATEST_MPATH=$(multipath -ll 2>/dev/null | grep -i "DellEMC,ME5" | awk '{print $1}' | head -1 || true)
            fi

            if [[ -z "${LATEST_MPATH}" ]]; then
              log "Selecting multipath using vendor match DellEMC,ME4"
              LATEST_MPATH=$(multipath -ll 2>/dev/null | grep -i "DellEMC,ME4" | awk '{print $1}' | head -1 || true)
            fi

            if [[ -z "${LATEST_MPATH}" ]]; then
              log "Selecting multipath using latest dm-* fallback"
              LATEST=$(multipath -ll 2>/dev/null | grep -oP 'dm-\d+' | sort -t- -k2 -n | tail -1 || true)
              if [[ -z "${LATEST}" ]]; then
                log "ERROR: No multipath dm-* devices detected"
                exit 1
              fi
              LATEST_MPATH=$(multipath -ll 2>/dev/null | grep "${LATEST}" | awk '{print $1}' | head -1 || true)
            fi

            if [[ -z "${LATEST_MPATH}" ]]; then
              log "ERROR: Unable to determine multipath device"
              exit 1
            fi

            MPATH_DEV="/dev/mapper/${LATEST_MPATH}"
            log "Using multipath device: ${MPATH_DEV}"

            PART_DEV="/dev/mapper/${LATEST_MPATH}1"

            if [[ ! -e "${PART_DEV}" ]]; then
              log "Creating GPT label and partition on ${MPATH_DEV}"
              parted -s "${MPATH_DEV}" mklabel gpt
              parted -s "${MPATH_DEV}" mkpart primary "${FS_TYPE}" 0% 100%
              sleep 2
              partprobe "${MPATH_DEV}" || true
              kpartx -av "${MPATH_DEV}" || true
              sleep 2
            fi

            log "Using partition device: ${PART_DEV}"

            if ! blkid -s TYPE -o value "${PART_DEV}" 2>/dev/null | grep -q .; then
              log "Formatting ${PART_DEV} with ${FS_TYPE}"
              mkfs."${FS_TYPE}" -f "${PART_DEV}"
            else
              log "Filesystem already exists on ${PART_DEV}; skipping format"
            fi

            mkdir -p "${PERSIST_MOUNT}"

            UUID=$(blkid -s UUID -o value "${PART_DEV}" 2>/dev/null || true)

            if [[ -n "${UUID}" ]]; then
              log "Using UUID=${UUID} for fstab"
              FSTAB_ENTRY="UUID=${UUID}"
              FSTAB_MATCH="^UUID=${UUID}\\s"
            else
              log "UUID not available, using device path ${PART_DEV} for fstab"
              FSTAB_ENTRY="${PART_DEV}"
              FSTAB_MATCH="^${PART_DEV}\\s"
            fi

            if ! grep -qE "${FSTAB_MATCH}" /etc/fstab; then
              log "Adding persistent mount to /etc/fstab"
              echo "${FSTAB_ENTRY} ${PERSIST_MOUNT} ${FS_TYPE} ${MOUNT_OPTS} 0 0" >> /etc/fstab
            fi

            if ! mountpoint -q "${PERSIST_MOUNT}"; then
              log "Mounting ${PERSIST_MOUNT}"
              mount "${PART_DEV}" "${PERSIST_MOUNT}"
            fi

            df -h "${PERSIST_MOUNT}" || true

            mkdir -p "${MYSQL_SUBDIR}" "${SPOOL_SUBDIR}" /var/lib/mysql /var/spool

            grep -qE "\s+/var/lib/mysql\s+none\s+bind" /etc/fstab || echo "${MYSQL_SUBDIR} /var/lib/mysql none bind 0 0" >> /etc/fstab
            grep -qE "\s+/var/spool\s+none\s+bind" /etc/fstab || echo "${SPOOL_SUBDIR} /var/spool none bind 0 0" >> /etc/fstab

            mount /var/lib/mysql || true
            mount /var/spool || true

            chown -R {{ mysql_user }}:{{ mysql_group }} /var/lib/mysql

            log "Final mount summary:"
            mount | grep -E "/mnt/slurm-persist|/var/lib/mysql|/var/spool" || true

            log "iSCSI sessions:"
            iscsiadm -m session || true

            log "Multipath status:"
            multipath -ll || true

            log "iSCSI/multipath setup complete. Log saved to ${LOGFILE}"
{% endif %}

{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '0600'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - path: /root/ldms_sampler.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/ldms/ldms_sampler.sh.j2') | indent(12) }}
{% endif %}

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /root/init_slurm_db.sql
          permissions: '{{ file_mode_600 }}'
          content: |
            SELECT VERSION();
            SHOW DATABASES;

            CREATE DATABASE slurm_acct_db;
            CREATE USER 'slurm'@'%' IDENTIFIED BY '{{ hostvars['localhost']['slurm_db_password'] }}';
            GRANT ALL PRIVILEGES ON slurm_acct_db.* TO 'slurm'@'%';
            FLUSH PRIVILEGES;

        - path: /root/omnia_slurm_scripts/00_munge_setup.sh
          permissions: '{{ file_mode_755 }}'
          content: |        
            chown -R {{ munge_user }}:{{ munge_group }} /etc/munge
            chmod 700 /etc/munge
            chmod {{ file_mode_400 }} /etc/munge/munge.key
            systemctl enable munge
            systemctl start munge

        - path: /root/omnia_slurm_scripts/01_mariadb_setup.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            SLURMDBD_CONF="/etc/slurm/slurmdbd.conf"
            SLURM_USER="{{ slurm_user }}"
            SLURM_GROUP="{{ slurm_user }}"
            # Function to extract value from slurm.conf 
            get_value_slurm_conf() {
                local key="$1"
                local default="$2"
                local value
                value=$(grep -iE "^\s*$key\s*=" "$SLURMDBD_CONF" | sed -E 's/^\s*[^=]+=//; s/#.*//; s/\s+$//')
                echo "${value:-$default}"
            }
            chown -R {{ mysql_user }}:{{ mysql_group }} /var/lib/mysql
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/log/mariadb
            chown -R {{ slurm_user }}:{{ slurm_user }} /etc/my.cnf.d # Required? why slurm user for my.cnf?? 
            chmod {{ file_mode_755 }} /etc/my.cnf.d /var/lib/mysql /var/log/mariadb
            #firewall
            systemctl enable firewalld
            systemctl start firewalld
            StoragePort=$(get_value_slurm_conf "StoragePort" "3306")
            firewall-cmd --permanent --add-port="$StoragePort"/tcp
            firewall-cmd --reload
            systemctl enable --now mariadb
            systemctl start mariadb
            mysql -u root < /root/init_slurm_db.sql                 

        - path: /root/omnia_slurm_scripts/02_slurmdbd_setup.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            SLURMDBD_CONF="/etc/slurm/slurmdbd.conf"
            SLURM_USER="{{ slurm_user }}"
            SLURM_GROUP="{{ slurm_user }}"
            # Function to extract value from slurm.conf 
            get_value_slurm_conf() {
                local key="$1"
                local default="$2"
                local value
                value=$(grep -iE "^\s*$key\s*=" "$SLURMDBD_CONF" | sed -E 's/^\s*[^=]+=//; s/#.*//; s/\s+$//')
                echo "${value:-$default}"
            }
            chmod {{ file_mode_600 }} /etc/slurm/slurmdbd.conf
            chown {{ slurm_user }}:{{ slurm_user }} /etc/slurm/slurmdbd.conf
            #file PidFile
            PidFile=$(get_value_slurm_conf "PidFile" "/var/run/slurmdbd.pid")
            mkdir -pv $(dirname "$PidFile")
            touch $PidFile
            chown -v "$SLURM_USER:$SLURM_GROUP" "$PidFile"
            chmod -v 0755 $PidFile
            #file LogFile
            LogFile=$(get_value_slurm_conf "LogFile" "/var/log/slurmdbd.log")
            mkdir -pv $(dirname "$LogFile")
            touch $LogFile
            chown -v "$SLURM_USER:$SLURM_GROUP" "$LogFile"
            chmod -v 0755 $LogFile
            #firewall
            systemctl enable firewalld
            systemctl start firewalld
            DbdPort=$(get_value_slurm_conf "DbdPort" "6819")
            firewall-cmd --permanent --add-port="$DbdPort"/tcp
            firewall-cmd --reload
            systemctl enable slurmdbd
            systemctl start slurmdbd

        - path: /root/omnia_slurm_scripts/03_slurmctld_setup.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            SLURM_CONF="/etc/slurm/slurm.conf"
            SLURM_USER="{{ slurm_user }}"
            SLURM_GROUP="{{ slurm_user }}"
            # Function to extract value from slurm.conf 
            get_value_slurm_conf() {
                local key="$1"
                local default="$2"
                local value
                value=$(grep -iE "^\s*$key\s*=" "$SLURM_CONF" | sed -E 's/^\s*[^=]+=//; s/#.*//; s/\s+$//')
                echo "${value:-$default}"
            }
            #dir StateSaveLocation
            StateSaveLocation=$(get_value_slurm_conf "StateSaveLocation" "/var/spool")
            mkdir -pv $StateSaveLocation
            chown -v "$SLURM_USER:$SLURM_GROUP" $StateSaveLocation
            chmod -v 0744 $StateSaveLocation
            #file SlurmctldPidFile
            SlurmctldPidFile=$(get_value_slurm_conf "SlurmctldPidFile" "/var/run/slurmctld.pid")
            mkdir -pv $(dirname "$SlurmctldPidFile")
            touch $SlurmctldPidFile
            chown -v "$SLURM_USER:$SLURM_GROUP" "$SlurmctldPidFile"
            chmod -v 0755 $SlurmctldPidFile
            #file SlurmctldLogFile
            SlurmctldLogFile=$(get_value_slurm_conf "SlurmctldLogFile" "/var/log/slurmctld.log")
            mkdir -pv $(dirname "$SlurmctldLogFile")
            touch $SlurmctldLogFile
            chown -v "$SLURM_USER:$SLURM_GROUP" "$SlurmctldLogFile"
            chmod -v 0755 $SlurmctldLogFile
            #firewall
            systemctl enable firewalld
            systemctl start firewalld
            SlurmctldPort=$(get_value_slurm_conf "SlurmctldPort" "6817")
            firewall-cmd --permanent --add-port="$SlurmctldPort"/tcp
            SrunPortRange=$(get_value_slurm_conf "SrunPortRange" "60001-63000")
            firewall-cmd --permanent --add-port="$SrunPortRange"/tcp
            firewall-cmd --reload
            systemctl enable slurmctld
            systemctl start slurmctld
            systemctl restart slurmctld

        - path: /root/omnia_slurm_scripts/04_track_file.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            MARKER="/var/log/track/slurm_controller_track"
            if [ -f "$MARKER" ]; then
              echo "Slurm controller track file already exists. Skipping."
              exit 0
            fi

            echo "Waiting for slurmctld to become active..."
            while true; do
              if systemctl is-active --quiet slurmctld; then
                echo "Slurm controller is active."
                touch "$MARKER"
                exit 0
              else
                echo "slurmctld is not active yet. Retrying in 5 seconds."
              fi
              sleep 5
            done

      runcmd:
         - /usr/local/bin/set-ssh.sh

         
         # Ensure Slurm NFS root is mounted at client_mount_path (e.g. /share_omnia)
         - mkdir -p {{ client_mount_path }}/slurm/ssh

        # slurm user and group created in the users module
        # Create directories for nfs and mount all
         - mkdir -p /var/log/slurm /etc/slurm {{ home_dir }} /etc/my.cnf.d /etc/munge /var/lib/mysql /var/log/mariadb /cert /var/log/track /var/lib/packages
         - echo "{{ cloud_init_nfs_path }}/cert  /cert   nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm      /etc/slurm       nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/my.cnf.d   /etc/my.cnf.d    nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/mariadb /var/log/mariadb nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
{% if powervault_config is not defined %}
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/lib/mysql  /var/lib/mysql   nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
{% endif %}
         - echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ trackfile_nfs_path }}    /var/log/track       nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/packages  /var/lib/packages   nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path }}/ssh {{ client_mount_path }}/slurm/ssh nfs defaults,_netdev 0 0" >> /etc/fstab
         - chmod {{ file_mode }} /etc/fstab
         - mount -a
         - cp /cert/pulp_webserver.crt /etc/pki/ca-trust/source/anchors && update-ca-trust
         - sed -i 's/^gpgcheck=1/gpgcheck=0/' /etc/dnf/dnf.conf
         - bash /usr/local/bin/doca-install.sh && bash /usr/local/bin/configure-ib-network.sh
{% if powervault_config is defined %}
         - /usr/local/bin/setup_iscsi_storage.sh
{% endif %}

         - chown -R {{ slurm_user }}:{{ slurm_user }} {{ home_dir }}
         - chmod {{ file_mode_755 }} {{ home_dir }}

         - chown -R {{ slurm_user }}:{{ slurm_user }} /etc/slurm
         - chmod {{ file_mode_755 }} /etc/slurm
         - chmod {{ file_mode }} /etc/slurm/slurm.conf

         - setenforce 0

         - ['bash', '/root/omnia_slurm_scripts/00_munge_setup.sh']
         - ['bash', '/root/omnia_slurm_scripts/01_mariadb_setup.sh']
         - ['bash', '/root/omnia_slurm_scripts/02_slurmdbd_setup.sh']
         - ['bash', '/root/omnia_slurm_scripts/03_slurmctld_setup.sh']
         - ['bash', '/root/omnia_slurm_scripts/04_track_file.sh']

         - sed -i 's/^PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
         - firewall-cmd --permanent --add-service=ssh
         - firewall-cmd --reload
         - systemctl enable sshd
         - systemctl start sshd

{% if hostvars['localhost']['openldap_support'] %}
         - /usr/local/bin/update_ldap_conf.sh
         - mkdir /ldapcerts
         - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
         - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
         - chmod {{ file_mode }} /etc/fstab
         - mount -a
         - yes | cp /ldapcerts/* /etc/openldap/certs
         - umount /ldapcerts

         - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
         - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
         - firewall-cmd --reload

         - authselect select sssd with-mkhomedir --force
         - sudo systemctl enable --now oddjobd.service
         - sudo systemctl enable --now sssd
         - setsebool -P authlogin_nsswitch_use_ldap on
         - setsebool -P authlogin_yubikey on
         - sudo systemctl restart sssd
         - systemctl restart sshd
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
         - echo " Starting LDMS setup " | tee -a /var/log/ldms-cloudinit.log

         # Add NFS entry and mount
         - mkdir -p {{ client_mount_path }}
         - echo "{{ cloud_init_slurm_nfs_path }} {{ client_mount_path }} nfs defaults,_netdev 0 0" >> /etc/fstab
         - mount -a

         - /root/ldms_sampler.sh
{% endif %}
         - echo "Cloud-Init has completed successfully."
