- name: {{ functional_group_name }}
  description: "{{ functional_group_name }}"

  file:
    encoding: plain
    content: |
      ## template: jinja
      #cloud-config
      merge_how:
      - name: list
        settings: [append]
      - name: dict
        settings: [no_replace, recurse_list]

      users:
        - name: root
          ssh_authorized_keys: "{{ read_ssh_key.stdout }}"
          lock_passwd: false
          hashed_passwd: "{{ hashed_password_output.stdout }}"
        - name: {{ slurm_user }}
          uid: {{ slurm_uid }}
          system: true
          no_create_home: true
          shell: /sbin/nologin
      disable_root: false

      write_files:
        - path: /usr/local/bin/doca-install.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/doca-install.sh.j2') | indent(12) }}

        - path: /usr/local/bin/configure-ib-network.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/doca-ofed/configure-ib-network.sh.j2') | indent(12) }}

        - path: /usr/local/bin/set-ssh.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            timedatectl set-timezone {{ hostvars['oim']['oim_timezone'] }}
            sed -i 's/^#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
            sed -i 's/^#PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config
            sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config.d/50-cloud-init.conf
            systemctl restart sshd
            default_count=$(ip route | grep -c "^default")
            if [ "$default_count" -le 1 ]; then
                echo "Only one or no default route found. No action needed."
            else
                private_nic=$(ip route | grep "^default via {{ hostvars['localhost']['admin_nic_ip'] }}" | awk '{print $5}')
                # Get all default routes
                ip route | grep '^default' | while read -r line; do
                    nmcli con del "Wired Connection"
                    # Extract NIC name
                    nic=$(echo "$line" | awk '{print $5}')

                    # Add the default route to the connection
                    if [ -n "$nic" ]; then
                        echo "Adding nmcli device $nic"
                        nmcli con add type ethernet ifname "$nic" con-name "$nic" ipv4.method auto
                        if [ "$nic" = "$private_nic" ]; then
                          nmcli con modify "$nic" ipv4.never-default yes
                          nmcli con delete "cloud-init $nic"
                        fi
                        nmcli con up "$nic"
                    else
                        echo "No connection found for device $nic"
                    fi
                done
            fi

        - path: /usr/local/bin/install_nvidia_driver.sh
          permissions: '0755'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/nvidia_install.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "===== Starting NVIDIA GPU detection and driver installation ====="

            # Check for NVIDIA GPU presence
            echo "[INFO] Checking for NVIDIA GPU..."
            if ! lspci | grep -i nvidia &>/dev/null; then
                echo "[INFO] No NVIDIA GPU detected. Exiting."
                exit 0
            fi

            echo "[INFO] NVIDIA GPU detected. Proceeding with setup."

            # Check if NVIDIA driver is already installed
            if command -v nvidia-smi &>/dev/null; then
                echo "[INFO] NVIDIA driver already installed. Skipping driver installation."
            else
                echo "[INFO] Mounting NFS runfile directory for driver installation..."
                mkdir -p /gpu-runfile
                mount -t nfs {{ cloud_init_nfs_path }}/hpc_tools/runfile /gpu-runfile

                if [ $? -ne 0 ]; then
                    echo "[ERROR] Failed to mount NFS runfile share. Exiting."
                    exit 1
                fi

                echo "[INFO] Installing NVIDIA driver..."
                if [ -f "/gpu-runfile/{{ cuda_runfile_aarch64 }}" ]; then
                    bash /gpu-runfile/{{ cuda_runfile_aarch64 }} --silent --driver --no-opengl-libs --kernel-source-path=/lib/modules/$(uname -r)/build
                    if [ $? -eq 0 ] && command -v nvidia-smi &>/dev/null; then
                        echo "[SUCCESS] NVIDIA driver installed successfully."
                    else
                        echo "[ERROR] NVIDIA driver installation failed."
                    fi
                else
                    echo "[ERROR] NVIDIA driver runfile not found in /gpu-runfile/"
                fi

                echo "[INFO] Cleaning up temporary NFS mount..."
                umount /gpu-runfile 2>/dev/null
                rmdir /gpu-runfile 2>/dev/null
            fi

            echo "[INFO] Setting up CUDA toolkit mount..."
            # Unmount first if already mounted
            umount /usr/local/cuda 2>/dev/null

            # Create mount point
            mkdir -p /usr/local/cuda

            cuda_nfs_share="{{ cloud_init_nfs_path }}/cuda"

            echo "[INFO] Mounting CUDA toolkit from NFS: $cuda_nfs_share"
            mount -t nfs "$cuda_nfs_share" /usr/local/cuda

            if [ $? -eq 0 ]; then
                echo "[SUCCESS] CUDA toolkit NFS mount successful"
                
                # Add to fstab for persistence
                grep -q "$cuda_nfs_share" /etc/fstab || echo "$cuda_nfs_share /usr/local/cuda nfs defaults,_netdev 0 0" >> /etc/fstab
                
                echo "[INFO] Configuring persistent CUDA environment..."
                
                # System-wide profile for login shells
                cat > /etc/profile.d/cuda.sh << 'EOF'
            export PATH=/usr/local/cuda/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
            export CUDA_HOME=/usr/local/cuda
            EOF
                chmod +x /etc/profile.d/cuda.sh
                
                # Bashrc for non-login shells
                cat > /etc/bashrc.cuda << 'EOF'
            if [ -d "/usr/local/cuda/bin" ]; then
                export PATH="/usr/local/cuda/bin:$PATH"
                export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
                export CUDA_HOME="/usr/local/cuda"
            fi
            EOF
                grep -q "bashrc.cuda" /etc/bashrc || echo "source /etc/bashrc.cuda" >> /etc/bashrc
                
                # Slurm prolog for job environment
                mkdir -p /etc/slurm/prolog.d
                cat > /etc/slurm/prolog.d/cuda.sh << 'EOF'
            #!/bin/bash
            export PATH=/usr/local/cuda/bin:$PATH
            export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
            export CUDA_HOME=/usr/local/cuda
            EOF
                chmod +x /etc/slurm/prolog.d/cuda.sh
                
                # Apply immediately for current session
                export PATH=/usr/local/cuda/bin:$PATH
                export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
                export CUDA_HOME=/usr/local/cuda
                
                echo "[SUCCESS] Persistent CUDA environment configured"
            else
                echo "[ERROR] Failed to mount CUDA toolkit NFS share"
                # Clean up failed mount
                rmdir /usr/local/cuda 2>/dev/null
                exit 1
            fi

            echo "[INFO] Verifying installation..."
            if command -v nvidia-smi &>/dev/null; then
                nvidia_version=$(nvidia-smi --version | head -n1)
                echo "[SUCCESS] NVIDIA driver: $nvidia_version"
            else
                echo "[ERROR] NVIDIA driver not found."
            fi

            if command -v nvcc &>/dev/null; then
                cuda_version=$(nvcc --version | grep "release" | awk '{print $6}' | sed 's/,//')
                echo "[SUCCESS] CUDA toolkit: version $cuda_version"
            else
                echo "[ERROR] CUDA toolkit (nvcc) not found."
            fi

            echo "[INFO] Testing persistence in new shell..."
            bash -c 'nvcc --version > /dev/null 2>&1'
            if [ $? -eq 0 ]; then
                echo "[SUCCESS] CUDA persistence test passed"
            else
                echo "[WARNING] CUDA persistence test failed - manual PATH setup may be needed"
            fi

            echo "===== NVIDIA GPU setup completed ====="

{% if hostvars['localhost']['openldap_support'] %}
        - path: /etc/sssd/sssd.conf
          owner: root:root
          permissions: '0600'
          content: |
            {{ lookup('template', 'templates/openldap/sssd.conf.j2') | indent(6) }}

        - path: /usr/local/bin/update_ldap_conf.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/openldap/update_ldap_conf.sh.j2') | indent(12) }}
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - path: /root/ldms_sampler.sh
          owner: root:root
          permissions: '0755'
          content: |
            {{ lookup('template', 'templates/ldms/ldms_sampler.sh.j2') | indent(12) }}
{% endif %}
        - path: /usr/local/bin/configure_dirs_and_mounts.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/configure_dirs_and_mounts.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "[INFO] ===== Starting directory creation and NFS mounts for Pulp cert, Slurm and Munge (aarch64) ====="

            echo "[INFO] Creating base directories for Slurm and Munge"
            mkdir -pv /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm /etc/slurm/epilog.d /etc/munge /cert /var/log/track /var/lib/packages /hpc_tools/container_images /hpc_tools/scripts

            echo "[INFO] Updating /etc/fstab with NFS entries for Pulp cert, Slurm and Munge paths"
            echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/log/slurm  /var/log/slurm   nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/slurm/epilog.d     /etc/slurm/epilog.d      nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/$(hostname -s)/var/spool      /var/spool       nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/$(hostname -s)/etc/munge      /etc/munge       nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ trackfile_nfs_path }}    /var/log/track       nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path}}/hpc_tools/container_images  /hpc_tools/container_images   nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path}}/hpc_tools/scripts  /hpc_tools/scripts   nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/cert  /cert   nfs defaults,_netdev 0 0" >> /etc/fstab
            echo "{{ cloud_init_nfs_path }}/packages  /var/lib/packages   nfs defaults,_netdev 0 0" >> /etc/fstab

            chmod {{ file_mode }} /etc/fstab

            echo "[INFO] Mounting all NFS entries from /etc/fstab"
            mount -av
            mkdir -p /etc/containers/registries.conf.d
            mv /tmp/apptainer_mirror.conf /etc/containers/registries.conf.d/apptainer_mirror.conf

            echo "[INFO] ===== Completed directory creation and NFS mounts for Slurm and Munge (aarch64) ====="

        - path: /usr/local/bin/configure_slurmd_setup.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/configure_slurmd_setup.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "[INFO] ===== Starting slurmd setup (service file, directories, epilog) (aarch64) ====="

            echo "[INFO] Copying slurmd.service into /usr/lib/systemd/system/"
            yes | cp /etc/slurm/epilog.d/slurmd.service /usr/lib/systemd/system/
            bash /usr/local/bin/check_slurm_controller_status.sh

            echo "[INFO] Setting ownership for Slurm directories"
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/log/slurm
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/run/slurm
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/spool
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/lib/slurm

            echo "[INFO] Setting permissions for Slurm directories"
            chmod {{ file_mode_755 }} /var/log/slurm /var/run/slurm /var/spool /var/lib/slurm

            echo "[INFO] Ensuring Slurm epilog directory and logout script permissions"
            chmod {{ file_mode_755 }} /etc/slurm/epilog.d/
            chmod {{ file_mode_755 }} /etc/slurm/epilog.d/logout_user.sh

            echo "[INFO] Creating and configuring /var/spool/slurmd"
            mkdir -p /var/spool/slurmd
            chmod {{ file_mode_755 }} /var/spool/slurmd
            chown -R {{ slurm_user }}:{{ slurm_user }} /var/spool/slurmd

            echo "[INFO] ===== Completed slurmd setup (aarch64) ====="

        - path: /usr/local/bin/configure_munge_and_pam.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/configure_munge_and_pam.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "[INFO] ===== Starting Munge key and PAM configuration (aarch64) ====="

            echo "[INFO] Setting ownership and permissions for Munge key"
            chown -R {{ munge_user }}:{{ munge_group }} /etc/munge/munge.key
            chmod {{ file_mode_400 }} /etc/munge/munge.key

            echo "[INFO] Updating PAM configuration for pam_slurm_adopt in /etc/pam.d/sshd"
            sed -i '/^password\s\+include\s\+password-auth/i account    required    pam_slurm_adopt.so action_no_jobs=deny' /etc/pam.d/sshd

            echo "[INFO] ===== Completed Munge key and PAM configuration (aarch64) ====="

        - path: /usr/local/bin/configure_firewall_and_services.sh
          permissions: '{{ file_mode_755 }}'
          content: |
            #!/bin/bash
            LOGFILE="/var/log/configure_firewall_and_services.log"
            exec > >(tee -a "$LOGFILE") 2>&1

            echo "[INFO] ===== Starting firewall and service configuration (aarch64) ====="

            echo "[INFO] Enabling and starting firewalld"
            systemctl enable firewalld
            systemctl start firewalld

            # Default values in case parsing slurm.conf fails
            DEFAULT_SRUN_RANGE="60001-63000"
            DEFAULT_SLURMD_PORT="6818"

            CTLD_SLURM_DIR_MNT="/mnt/slurm_ctld_etc_slurm"
            SLURM_CONF_PATH="$CTLD_SLURM_DIR_MNT/slurm.conf"

            echo "[INFO] Mounting controller slurm.conf from NFS: {{ cloud_init_nfs_path }}/{{ ctld_list[0] }}/etc/slurm -> $CTLD_SLURM_DIR_MNT"
            mkdir -p "$CTLD_SLURM_DIR_MNT"
            mount -t nfs "{{ cloud_init_nfs_path }}/{{ ctld_list[0] }}/etc/slurm" "$CTLD_SLURM_DIR_MNT" || {
              echo "[WARN] Failed to mount controller slurm.conf directory, falling back to defaults."
              SRUN_RANGE="$DEFAULT_SRUN_RANGE"
              SLURMD_PORT="$DEFAULT_SLURMD_PORT"
            }

            if [ -f "$SLURM_CONF_PATH" ]; then
              echo "[INFO] Parsing SlurmdPort and SrunPortRange from $SLURM_CONF_PATH"

              SLURMD_PORT=$(grep -iE '^SlurmdPort=' "$SLURM_CONF_PATH" | sed -E 's/^SlurmdPort=//; s/#.*//; s/\s+$//')
              SRUN_RANGE=$(grep -iE '^SrunPortRange=' "$SLURM_CONF_PATH" | sed -E 's/^SrunPortRange=//; s/#.*//; s/\s+$//')

              [ -z "$SLURMD_PORT" ] && SLURMD_PORT="$DEFAULT_SLURMD_PORT" && echo "[WARN] SlurmdPort not found in slurm.conf, using default $SLURMD_PORT"
              [ -z "$SRUN_RANGE" ] && SRUN_RANGE="$DEFAULT_SRUN_RANGE" && echo "[WARN] SrunPortRange not found in slurm.conf, using default $SRUN_RANGE"
            else
              echo "[WARN] slurm.conf not found at $SLURM_CONF_PATH, using defaults."
              SRUN_RANGE="$DEFAULT_SRUN_RANGE"
              SLURMD_PORT="$DEFAULT_SLURMD_PORT"
            fi

            echo "[INFO] Using SlurmdPort=$SLURMD_PORT and SrunPortRange=$SRUN_RANGE for firewall configuration"

            echo "[INFO] Configuring firewall rules for SSH and Slurm ports"
            firewall-cmd --permanent --add-service=ssh
            firewall-cmd --permanent --add-port="${SRUN_RANGE}"/tcp
            firewall-cmd --permanent --add-port="${SLURMD_PORT}"/tcp
            firewall-cmd --reload

            echo "[INFO] Unmounting controller slurm.conf directory from $CTLD_SLURM_DIR_MNT"
            umount "$CTLD_SLURM_DIR_MNT" 2>/dev/null || echo "[WARN] Failed to unmount $CTLD_SLURM_DIR_MNT (may not have been mounted)"

            echo "[INFO] Enabling and starting core services: sshd, munge, slurmd"
            systemctl enable sshd
            systemctl start sshd
            systemctl enable munge
            systemctl start munge
            systemctl enable slurmd
            systemctl start slurmd

            echo "[INFO] Reloading systemd daemon and restarting sshd"
            systemctl daemon-reexec
            systemctl restart sshd
            systemctl restart slurmd

            echo "[INFO] ===== Completed firewall and service configuration (aarch64) ====="

        - path: /etc/hosts
          append: true
          content: |
{% for key in ip_name_map | sort %}
            {{ ip_name_map[key] }} {{ key }}
{% endfor %}

        - path: /usr/local/bin/check_slurm_controller_status.sh
          owner: root:root
          permissions: '{{ file_mode_755 }}'
          content: |
            {{ lookup('template', 'templates/slurm/check_slurm_controller_status.sh.j2') | indent(12) }}

        - path: /tmp/apptainer_mirror.conf
          permissions: '0644'
          content: |
            {{ lookup('template', 'templates/nodes/apptainer_mirror.conf.j2') | indent(12) }}

      runcmd:
        - /usr/local/bin/set-ssh.sh
        - /usr/local/bin/install_nvidia_driver.sh

        - /usr/local/bin/configure_dirs_and_mounts.sh
        - cp /cert/pulp_webserver.crt /etc/pki/ca-trust/source/anchors && update-ca-trust
        - sed -i 's/^gpgcheck=1/gpgcheck=0/' /etc/dnf/dnf.conf
        - bash /usr/local/bin/doca-install.sh && bash /usr/local/bin/configure-ib-network.sh
        - /usr/local/bin/configure_slurmd_setup.sh
        - /usr/local/bin/configure_munge_and_pam.sh

        - setenforce 0
        - /usr/local/bin/configure_firewall_and_services.sh

{% if hostvars['localhost']['openldap_support'] %}
        - /usr/local/bin/update_ldap_conf.sh
        - mkdir /ldapcerts
        - echo "{{ cloud_init_nfs_path_openldap }}/certs                /ldapcerts       nfs defaults,_netdev 0 0" >> /etc/fstab
        - echo "{{ cloud_init_nfs_path_openldap }}/ldapuser             /home            nfs defaults,_netdev 0 0" >> /etc/fstab
        - chmod {{ file_mode }} /etc/fstab
        - mount -a
        - yes | cp /ldapcerts/* /etc/openldap/certs
        - umount /ldapcerts

        - firewall-cmd --permanent --add-port={{ ldap_starttls_port }}/tcp
        - firewall-cmd --permanent --add-port={{ ldap_ssl_port }}/tcp
        - firewall-cmd --reload

        - setenforce 0
        - authselect select sssd with-mkhomedir --force
        - sudo systemctl enable --now oddjobd.service
        - sudo systemctl enable --now sssd
        - setsebool -P authlogin_nsswitch_use_ldap on
        - setsebool -P authlogin_yubikey on
        - sudo systemctl restart sssd
        - systemctl restart sshd

{% endif %}

{% if hostvars['localhost']['ucx_support'] or hostvars['localhost']['openmpi_support'] or hostvars['localhost']['ldms_support'] %}
        # Add NFS entry and mount
        - mkdir -p {{ client_mount_path }}
        - echo "{{ cloud_init_slurm_nfs_path }} {{ client_mount_path }} nfs defaults,_netdev 0 0" >> /etc/fstab
        - mount -a
{% endif %}

{% if hostvars['localhost']['ldms_support'] %}
        - echo " Starting LDMS setup " | tee -a /var/log/ldms-cloudinit.log

        - /root/ldms_sampler.sh
{% endif %}
        - echo "Cloud-Init has completed successfully."
