# Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

# Usage: discover_mapping_nodes.yml
openchami_nodes_template: "{{ role_path }}/templates/nodes/nodes.yaml.j2"
bmc_group_data_template: "{{ role_path }}/templates/nodes/bmc_group_data.csv.j2"
openchami_share_path: /opt/omnia/openchami
telemetry_share_path: "{{ hostvars['localhost']['oim_shared_path'] }}/omnia/telemetry"
bmc_group_data_file: "{{ telemetry_share_path }}/bmc_group_data.csv"
openchami_work_dir: "{{ hostvars['localhost']['oim_shared_path'] }}/omnia/openchami/workdir"
nodes_dir: "{{ openchami_work_dir }}/nodes"
openchami_config_vars_path: "{{ openchami_share_path }}/configs_vars.yaml"
openchami_groups_template: "{{ role_path }}/templates/nodes/groups.yaml.j2"
openchami_nodes_vars_path: "{{ nodes_dir }}/nodes.yaml"
discover_fail_msg: "Failed to discover nodes. Please verify the inputs provided in mapping file."
openchami_hostname_template: "{{ role_path }}/templates/nodes/hostname.yaml.j2"
openchami_hostname_vars_path: "{{ nodes_dir }}/hostname.yaml"

# Usage: create_groups.yml
openchami_groups_common_template: "{{ role_path }}/templates/nodes/groups_common.yaml.j2"
common_cloud_init_groups:
  - ssh
  - chrony

# Usage: configure_bss_group.yml, configure_bss_cloud_init.yml
bss_template: bss/bss.yaml.j2
bss_dir: "{{ openchami_work_dir }}/boot"
bss_params_cloud_init: 'ds=nocloud;s=http://{{ cluster_boot_ip }}:8081/cloud-init/'
bss_params_opts: 'ip=dhcp rd.live.image rd.live.ram rd.neednet=1 rd.driver.blacklist=ccp,edac_core,power_meter,ahci,megaraid_sas modprobe.blacklist=ccp,edac_core,power_meter,ahci,megaraid_sas libata.force=1:disable,2:disable,3:disable,4:disable rd.luks=0 rd.md=0 rd.dm=0 console=tty0 console=ttyS0,115200 selinux=0 apparmor=0 ip6=off cloud-init=enabled' # noqa: yaml[line-length]
image_missing_fail_msg: "Failed to set kernel or initrd. Create the image using build_image.yml and try again."

# Usage: configure_cloud_init_group.yml, configure_bss_cloud_init.yml
ssh_key_path: /root/.ssh/oim_rsa.pub
ci_defaults_template: cloud_init/ci-defaults.yaml.j2
cloud_init_dir: "{{ openchami_work_dir }}/cloud-init"
ci_defaults_dest: '{{ cloud_init_dir }}/ci-defaults.yaml'
ci_group_load_fail_msg: |
  "Template loading failed. Ensure the template exists in the specified path and is compatible with the defined functional groups."
default_file_path: "{{ playbook_dir }}/roles/slurm_config/defaults/main.yml"
ssh_private_key_path: /root/.ssh/oim_rsa

# Usage: configure_cloud_init_common.yml
ci_group_common_template: cloud_init/ci-group-common.yaml.j2
ci_group_common_dest: "{{ cloud_init_dir }}/ci-group-common.yaml"

# Usage: discovery_completion.yml
discovery_completion_msg: |
  The discovery.yml playbook has completed successfully.
  Next, you can either manually PXE boot the nodes or use the utils/set_pxe_boot.yml playbook
  by specifying a bmc group in your inventory to initiate the PXE boot process.
  Once the nodes have booted, proceed to run telemetry/telemetry.yml to start collecting telemetry data.

# Usage: ci-group-login_node_x86_64.yaml.j2
tls_cert_path: "/etc/openldap/certs/ldapserver.crt"
ldap_conf_dest: "/etc/openldap/ldap.conf"
sasl_nocanon_regxp: "SASL_NOCANON\ton"
sasl_nacanon_replace1: "SASL_NOCANON\ton\nBASE\t{{ hostvars['localhost']['ldap_search_base'] }}"
sasl_nacanon_replace2: "SASL_NOCANON\ton\nURI\tldap://{{ hostvars['localhost']['ldap_server_ip'] }}"
sasl_nacanon_replace3: "SASL_NOCANON\ton\nTLS_CACERT\t{{ tls_cert_path }}"
sasl_nacanon_replace4: "SASL_NOCANON\ton\nURI\tldap://{{ hostvars['localhost']['ldap_server_ip'] }}:636"
file_mode: "0644"
ldap_starttls_port: 389
ldap_ssl_port: 636

# Usage: ci-group-slurm_control_node_x86_64.yaml.j2
home_dir: /var/lib/slurm
user: slurm
munge_user: munge
munge_group: munge
mysql_user: mysql
mysql_group: mysql
file_mode_400: "0400"
file_mode_755: "0755"
file_mode_600: "0600"
ip_timeout: 10
ip_wait_loop: 60

# Hostname lists for stack-specific SSH configs (populated by passwordless_ssh role)
k8s_cluster_hostnames: "{{ hostvars['localhost']['k8s_cluster_hostnames'] | default([]) }}"
slurm_cluster_hostnames: "{{ hostvars['localhost']['slurm_cluster_hostnames'] | default([]) }}"

# IP wildcard lists for stack-specific SSH configs
k8s_cluster_ip_patterns: "{{ hostvars['localhost']['k8s_cluster_ip_patterns'] | default([]) }}"
slurm_cluster_ip_patterns: "{{ hostvars['localhost']['slurm_cluster_ip_patterns'] | default([]) }}"

# SSH Host patterns precomputed on OIM by passwordless_ssh/read_nodes_yaml.yml
slurm_control_ssh_patterns: "{{ hostvars['oim']['slurm_ssh_patterns'] | default('*') }}"
k8s_control_ssh_patterns: "{{ hostvars['oim']['k8s_ssh_patterns'] | default('*') }}"

# Passwordless SSH mode flag derived from nodes.yaml (set on OIM by passwordless_ssh role)
all_group_names_present: "{{ hostvars['oim']['all_group_names_present'] | default(false) }}"

# CUDA/NVIDIA runfile names (extracted from slurm_custom.json in slurm_config role)
cuda_runfile_x86_64: "{{ hostvars['oim']['cuda_runfile_x86_64'] | default('cuda_13.0.2_580.95.05_linux.run') }}"
cuda_runfile_aarch64: "{{ hostvars['oim']['cuda_runfile_aarch64'] | default('cuda_13.0.2_580.95.05_linux_sbsa.run') }}"
