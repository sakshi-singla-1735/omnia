---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: pod-cleanup
  namespace: telemetry
spec:
  schedule: "*/3 * * * *" # Every 3 minutes
  successfulJobsHistoryLimit: 1   # Keep only 1 successful job
  failedJobsHistoryLimit: 1       # Keep only 1 failed job
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 60 # Auto-delete job and pod after 60s
      activeDeadlineSeconds: 180 # Kill job if it runs longer than 3 minutes
      template:
        spec:
          tolerations:
          - effect: NoExecute
            key: node.kubernetes.io/not-ready
            operator: Exists
            tolerationSeconds: 30  # Evict after 30s if node is not ready
          - effect: NoExecute
            key: node.kubernetes.io/unreachable
            operator: Exists
            tolerationSeconds: 30  # Evict after 30s if node is unreachable
          containers:
          - name: kubectl-cleanup
            image: docker.io/alpine/kubectl:1.34.1
            command:
            - /bin/sh
            - -c
            - |
              apk add --no-cache coreutils
              set -e

              # Get all terminating pods
              terminating=$(kubectl get pods -n telemetry -o jsonpath='{range .items[?(@.metadata.deletionTimestamp)]}{.metadata.name}{"\n"}{end}')
              
              if [ -z "$terminating" ]; then
                echo "No terminating pods found"
              else
              now=$(date +%s)
                
                for pod in $terminating; do
                  deletion_ts=$(kubectl get pod "$pod" -n telemetry -o jsonpath='{.metadata.deletionTimestamp}' 2>/dev/null)
                  if [ -z "$deletion_ts" ]; then
                    continue
                  fi
                  
                  deletion_time=$(date -d "$deletion_ts" +%s || echo 0)
                  age=$((now - deletion_time))
                  
                  # Check age threshold (300s for Kafka, 60s for others)
                  threshold=60
                  if echo "$pod" | grep -q "kafka"; then
                    threshold=300
                  fi
                  
                  if [ $age -gt $threshold ]; then
                    echo "→ Pod $pod stuck for $age seconds. Processing..."
                    
                    # Get PVCs BEFORE deleting pod
                    pvcs=$(kubectl get pod "$pod" -n telemetry -o jsonpath='{.spec.volumes[*].persistentVolumeClaim.claimName}' 2>/dev/null)
                    
                    # For Kafka: scale down to prevent immediate recreation
                    if echo "$pod" | grep -q "kafka-broker"; then
                      broker_num=$(echo "$pod" | sed 's/.*-\([0-9]*\)$/\1/')
                      echo "  Scaling down Kafka broker nodepool temporarily..."
                      kubectl patch kafkanodepool broker -n telemetry --type=merge -p "{\"spec\":{\"replicas\":0}}" 2>/dev/null || true
                    elif echo "$pod" | grep -q "kafka-controller"; then
                      echo "  Scaling down Kafka controller nodepool temporarily..."
                      kubectl patch kafkanodepool controller -n telemetry --type=merge -p "{\"spec\":{\"replicas\":0}}" 2>/dev/null || true
                    fi
                    
                    # Delete pod
                    if ! echo "$pod" | grep -q "kafka"; then
                      kubectl patch pod "$pod" -n telemetry -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
                    fi
                    kubectl delete pod "$pod" -n telemetry --grace-period=0 --force 2>/dev/null || true
                    
                    # Clean PVCs if any
                    if [ -n "$pvcs" ]; then
                      for pvc in $pvcs; do
                        echo "  → Cleaning $pvc..."
                        cleanup_pod="clean-$RANDOM"
                        
                        kubectl run $cleanup_pod --image=busybox:1.36 -n telemetry --restart=Never \
                          --overrides="{\"spec\":{\"containers\":[{\"name\":\"cleanup\",\"image\":\"busybox:1.36\",\"command\":[\"/bin/sh\",\"-c\",\"find /data -type f \\\\( -name .lock -o -name '*.sock' -o -name '*.pid' \\\\) -exec rm -fv {} \\\\; || true\"],\"volumeMounts\":[{\"name\":\"data\",\"mountPath\":\"/data\"}]}],\"volumes\":[{\"name\":\"data\",\"persistentVolumeClaim\":{\"claimName\":\"$pvc\"}}]}}" \
                          2>/dev/null || echo "  ✗ Failed to create cleanup pod"
                        
                        # Wait for cleanup to complete (max 15s)
                        kubectl wait --for=condition=Ready pod/$cleanup_pod -n telemetry --timeout=15s 2>/dev/null || echo "  ⚠ Cleanup pod still pending"
                        kubectl logs $cleanup_pod -n telemetry 2>/dev/null || true
                        kubectl delete pod $cleanup_pod -n telemetry 2>/dev/null || true
                      done
                      echo "  ✓ Cleaned PVCs for $pod"
                      
                      # Scale Kafka back up
                      if echo "$pod" | grep -q "kafka-broker"; then
                        echo "  Scaling Kafka broker nodepool back up..."
                        kubectl patch kafkanodepool broker -n telemetry --type=merge -p '{"spec":{"replicas":{{ kafka.node_pools.broker.replicas }}}}' 2>/dev/null || true
                      elif echo "$pod" | grep -q "kafka-controller"; then
                        echo "  Scaling Kafka controller nodepool back up..."
                        kubectl patch kafkanodepool controller -n telemetry --type=merge -p '{"spec":{"replicas":{{ kafka.node_pools.controller.replicas }}}}' 2>/dev/null || true
                      fi
                    fi
              else
                    echo "→ Pod $pod terminating for $age seconds (threshold: ${threshold}s). Skipping."
                  fi
                done
              fi
              
              echo "Cleanup complete"
              
              exit 0
          restartPolicy: Never
          serviceAccountName: telemetry-cleaner
